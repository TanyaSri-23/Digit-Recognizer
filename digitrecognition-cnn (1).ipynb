{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:05.424338Z",
     "iopub.status.busy": "2021-12-10T03:50:05.42395Z",
     "iopub.status.idle": "2021-12-10T03:50:05.430958Z",
     "shell.execute_reply": "2021-12-10T03:50:05.430065Z",
     "shell.execute_reply.started": "2021-12-10T03:50:05.424293Z"
    },
    "id": "ro9B5yFIU6r3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:05.449435Z",
     "iopub.status.busy": "2021-12-10T03:50:05.448914Z",
     "iopub.status.idle": "2021-12-10T03:50:09.752246Z",
     "shell.execute_reply": "2021-12-10T03:50:09.751394Z",
     "shell.execute_reply.started": "2021-12-10T03:50:05.449398Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\Tanya srivastava\\Desktop\\train_digits.csv\")\n",
    "X_test = pd.read_csv(r\"C:\\Users\\Tanya srivastava\\Desktop\\test_digits.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:09.754618Z",
     "iopub.status.busy": "2021-12-10T03:50:09.754341Z",
     "iopub.status.idle": "2021-12-10T03:50:09.770067Z",
     "shell.execute_reply": "2021-12-10T03:50:09.768813Z",
     "shell.execute_reply.started": "2021-12-10T03:50:09.754586Z"
    }
   },
   "outputs": [],
   "source": [
    "# sub = pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:09.771845Z",
     "iopub.status.busy": "2021-12-10T03:50:09.771605Z",
     "iopub.status.idle": "2021-12-10T03:50:09.778082Z",
     "shell.execute_reply": "2021-12-10T03:50:09.777265Z",
     "shell.execute_reply.started": "2021-12-10T03:50:09.771816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:09.780603Z",
     "iopub.status.busy": "2021-12-10T03:50:09.779586Z",
     "iopub.status.idle": "2021-12-10T03:50:10.223501Z",
     "shell.execute_reply": "2021-12-10T03:50:10.222679Z",
     "shell.execute_reply.started": "2021-12-10T03:50:09.780554Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, y_train = train.iloc[:, 1:].values , train.iloc[:, 0].values\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = .25)\n",
    "x_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.225087Z",
     "iopub.status.busy": "2021-12-10T03:50:10.224784Z",
     "iopub.status.idle": "2021-12-10T03:50:10.23149Z",
     "shell.execute_reply": "2021-12-10T03:50:10.230481Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.225046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31500, 784)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.233244Z",
     "iopub.status.busy": "2021-12-10T03:50:10.232934Z",
     "iopub.status.idle": "2021-12-10T03:50:10.242763Z",
     "shell.execute_reply": "2021-12-10T03:50:10.24195Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.233202Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotting(x,y,index):\n",
    "    plt.imshow(x[index])\n",
    "    plt.xlabel(y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.244816Z",
     "iopub.status.busy": "2021-12-10T03:50:10.244291Z",
     "iopub.status.idle": "2021-12-10T03:50:10.405695Z",
     "shell.execute_reply": "2021-12-10T03:50:10.405093Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.24476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotting(x_train.reshape((31500,28,28)), y_train, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.409678Z",
     "iopub.status.busy": "2021-12-10T03:50:10.409192Z",
     "iopub.status.idle": "2021-12-10T03:50:10.414822Z",
     "shell.execute_reply": "2021-12-10T03:50:10.413941Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.409631Z"
    },
    "id": "BkRv4kThW5bK"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "LR = 0.001\n",
    "epochs = 10\n",
    "n_classes = 10\n",
    "in_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalising Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.416241Z",
     "iopub.status.busy": "2021-12-10T03:50:10.416018Z",
     "iopub.status.idle": "2021-12-10T03:50:10.62441Z",
     "shell.execute_reply": "2021-12-10T03:50:10.62366Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.416214Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train/255\n",
    "x_val = x_val/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.626327Z",
     "iopub.status.busy": "2021-12-10T03:50:10.625849Z",
     "iopub.status.idle": "2021-12-10T03:50:10.630816Z",
     "shell.execute_reply": "2021-12-10T03:50:10.63013Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.62629Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1,28,28)\n",
    "x_val = x_val.reshape(-1,28,28)\n",
    "x_test = x_test.reshape(-1,28,28)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.632595Z",
     "iopub.status.busy": "2021-12-10T03:50:10.632168Z",
     "iopub.status.idle": "2021-12-10T03:50:10.735171Z",
     "shell.execute_reply": "2021-12-10T03:50:10.734511Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.632563Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(x_train).type(torch.float32).unsqueeze(1)\n",
    "\n",
    "x_val = torch.from_numpy(x_val).type(torch.float32).unsqueeze(1)\n",
    "x_test = torch.from_numpy(x_test).type(torch.float32).unsqueeze(1)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_val = torch.from_numpy(y_val)\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "val_ds = TensorDataset(x_val, y_val)\n",
    "train_loader = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_loader = DataLoader(val_ds, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.73692Z",
     "iopub.status.busy": "2021-12-10T03:50:10.736508Z",
     "iopub.status.idle": "2021-12-10T03:50:10.74715Z",
     "shell.execute_reply": "2021-12-10T03:50:10.746295Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.736889Z"
    },
    "id": "F8FFL2uzXFQO"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels =1, out_channels =6, kernel_size = (3,3), stride=(1,1), padding=(1,1))\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size= (2,2), stride= (2,2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels =16, kernel_size = (3,3), stride=(1,1), padding=(1,1))\n",
    "        self.fc1 = nn.Linear(16*7*7, 120)\n",
    "\n",
    "        self.fc2 = nn.Linear(120, 80)\n",
    "        self.fc3 = nn.Linear(80, 10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = x.view(-1, 16*7*7)\n",
    "        #print( x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.748865Z",
     "iopub.status.busy": "2021-12-10T03:50:10.748321Z",
     "iopub.status.idle": "2021-12-10T03:50:10.767628Z",
     "shell.execute_reply": "2021-12-10T03:50:10.766746Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.748825Z"
    },
    "id": "CFzBgMjCXH0s"
   },
   "outputs": [],
   "source": [
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=80, bias=True)\n",
      "  (fc3): Linear(in_features=80, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.769934Z",
     "iopub.status.busy": "2021-12-10T03:50:10.769288Z",
     "iopub.status.idle": "2021-12-10T03:50:10.780779Z",
     "shell.execute_reply": "2021-12-10T03:50:10.77995Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.769879Z"
    },
    "id": "j9dyv0tnXK6z"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:10.782567Z",
     "iopub.status.busy": "2021-12-10T03:50:10.78223Z",
     "iopub.status.idle": "2021-12-10T03:50:28.931912Z",
     "shell.execute_reply": "2021-12-10T03:50:28.930871Z",
     "shell.execute_reply.started": "2021-12-10T03:50:10.78252Z"
    },
    "id": "_sG_9wweXNFR",
    "outputId": "7cd6cc50-2b1a-4fdf-8388-ba850a1f134c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n",
      "Epoch [1/10], Step [1/124], Loss: 0.0285\n",
      "Epoch [1/10], Step [2/124], Loss: 0.0504\n",
      "Epoch [1/10], Step [3/124], Loss: 0.0313\n",
      "Epoch [1/10], Step [4/124], Loss: 0.0542\n",
      "Epoch [1/10], Step [5/124], Loss: 0.0358\n",
      "Epoch [1/10], Step [6/124], Loss: 0.0352\n",
      "Epoch [1/10], Step [7/124], Loss: 0.0365\n",
      "Epoch [1/10], Step [8/124], Loss: 0.0941\n",
      "Epoch [1/10], Step [9/124], Loss: 0.0505\n",
      "Epoch [1/10], Step [10/124], Loss: 0.0602\n",
      "Epoch [1/10], Step [11/124], Loss: 0.0585\n",
      "Epoch [1/10], Step [12/124], Loss: 0.0471\n",
      "Epoch [1/10], Step [13/124], Loss: 0.0403\n",
      "Epoch [1/10], Step [14/124], Loss: 0.0860\n",
      "Epoch [1/10], Step [15/124], Loss: 0.0531\n",
      "Epoch [1/10], Step [16/124], Loss: 0.0464\n",
      "Epoch [1/10], Step [17/124], Loss: 0.0513\n",
      "Epoch [1/10], Step [18/124], Loss: 0.1077\n",
      "Epoch [1/10], Step [19/124], Loss: 0.0292\n",
      "Epoch [1/10], Step [20/124], Loss: 0.0387\n",
      "Epoch [1/10], Step [21/124], Loss: 0.0428\n",
      "Epoch [1/10], Step [22/124], Loss: 0.0589\n",
      "Epoch [1/10], Step [23/124], Loss: 0.0400\n",
      "Epoch [1/10], Step [24/124], Loss: 0.0648\n",
      "Epoch [1/10], Step [25/124], Loss: 0.0621\n",
      "Epoch [1/10], Step [26/124], Loss: 0.0602\n",
      "Epoch [1/10], Step [27/124], Loss: 0.0438\n",
      "Epoch [1/10], Step [28/124], Loss: 0.0604\n",
      "Epoch [1/10], Step [29/124], Loss: 0.0535\n",
      "Epoch [1/10], Step [30/124], Loss: 0.0491\n",
      "Epoch [1/10], Step [31/124], Loss: 0.0523\n",
      "Epoch [1/10], Step [32/124], Loss: 0.0337\n",
      "Epoch [1/10], Step [33/124], Loss: 0.0708\n",
      "Epoch [1/10], Step [34/124], Loss: 0.0294\n",
      "Epoch [1/10], Step [35/124], Loss: 0.0466\n",
      "Epoch [1/10], Step [36/124], Loss: 0.0543\n",
      "Epoch [1/10], Step [37/124], Loss: 0.0567\n",
      "Epoch [1/10], Step [38/124], Loss: 0.0614\n",
      "Epoch [1/10], Step [39/124], Loss: 0.0456\n",
      "Epoch [1/10], Step [40/124], Loss: 0.0256\n",
      "Epoch [1/10], Step [41/124], Loss: 0.0302\n",
      "Epoch [1/10], Step [42/124], Loss: 0.0164\n",
      "Epoch [1/10], Step [43/124], Loss: 0.0623\n",
      "Epoch [1/10], Step [44/124], Loss: 0.0348\n",
      "Epoch [1/10], Step [45/124], Loss: 0.0564\n",
      "Epoch [1/10], Step [46/124], Loss: 0.0753\n",
      "Epoch [1/10], Step [47/124], Loss: 0.0917\n",
      "Epoch [1/10], Step [48/124], Loss: 0.0501\n",
      "Epoch [1/10], Step [49/124], Loss: 0.0309\n",
      "Epoch [1/10], Step [50/124], Loss: 0.0533\n",
      "Epoch [1/10], Step [51/124], Loss: 0.0265\n",
      "Epoch [1/10], Step [52/124], Loss: 0.0355\n",
      "Epoch [1/10], Step [53/124], Loss: 0.0658\n",
      "Epoch [1/10], Step [54/124], Loss: 0.0668\n",
      "Epoch [1/10], Step [55/124], Loss: 0.0342\n",
      "Epoch [1/10], Step [56/124], Loss: 0.0460\n",
      "Epoch [1/10], Step [57/124], Loss: 0.0559\n",
      "Epoch [1/10], Step [58/124], Loss: 0.0497\n",
      "Epoch [1/10], Step [59/124], Loss: 0.0553\n",
      "Epoch [1/10], Step [60/124], Loss: 0.0481\n",
      "Epoch [1/10], Step [61/124], Loss: 0.0698\n",
      "Epoch [1/10], Step [62/124], Loss: 0.0262\n",
      "Epoch [1/10], Step [63/124], Loss: 0.0511\n",
      "Epoch [1/10], Step [64/124], Loss: 0.0448\n",
      "Epoch [1/10], Step [65/124], Loss: 0.0266\n",
      "Epoch [1/10], Step [66/124], Loss: 0.0498\n",
      "Epoch [1/10], Step [67/124], Loss: 0.0294\n",
      "Epoch [1/10], Step [68/124], Loss: 0.0378\n",
      "Epoch [1/10], Step [69/124], Loss: 0.0586\n",
      "Epoch [1/10], Step [70/124], Loss: 0.0234\n",
      "Epoch [1/10], Step [71/124], Loss: 0.0685\n",
      "Epoch [1/10], Step [72/124], Loss: 0.1065\n",
      "Epoch [1/10], Step [73/124], Loss: 0.0337\n",
      "Epoch [1/10], Step [74/124], Loss: 0.0397\n",
      "Epoch [1/10], Step [75/124], Loss: 0.0407\n",
      "Epoch [1/10], Step [76/124], Loss: 0.0905\n",
      "Epoch [1/10], Step [77/124], Loss: 0.0723\n",
      "Epoch [1/10], Step [78/124], Loss: 0.0291\n",
      "Epoch [1/10], Step [79/124], Loss: 0.0614\n",
      "Epoch [1/10], Step [80/124], Loss: 0.0418\n",
      "Epoch [1/10], Step [81/124], Loss: 0.0450\n",
      "Epoch [1/10], Step [82/124], Loss: 0.0359\n",
      "Epoch [1/10], Step [83/124], Loss: 0.0449\n",
      "Epoch [1/10], Step [84/124], Loss: 0.0712\n",
      "Epoch [1/10], Step [85/124], Loss: 0.0228\n",
      "Epoch [1/10], Step [86/124], Loss: 0.0400\n",
      "Epoch [1/10], Step [87/124], Loss: 0.0760\n",
      "Epoch [1/10], Step [88/124], Loss: 0.0125\n",
      "Epoch [1/10], Step [89/124], Loss: 0.0596\n",
      "Epoch [1/10], Step [90/124], Loss: 0.0204\n",
      "Epoch [1/10], Step [91/124], Loss: 0.0342\n",
      "Epoch [1/10], Step [92/124], Loss: 0.0693\n",
      "Epoch [1/10], Step [93/124], Loss: 0.0346\n",
      "Epoch [1/10], Step [94/124], Loss: 0.0770\n",
      "Epoch [1/10], Step [95/124], Loss: 0.0497\n",
      "Epoch [1/10], Step [96/124], Loss: 0.0705\n",
      "Epoch [1/10], Step [97/124], Loss: 0.0592\n",
      "Epoch [1/10], Step [98/124], Loss: 0.0258\n",
      "Epoch [1/10], Step [99/124], Loss: 0.0343\n",
      "Epoch [1/10], Step [100/124], Loss: 0.0230\n",
      "Epoch [1/10], Step [101/124], Loss: 0.0238\n",
      "Epoch [1/10], Step [102/124], Loss: 0.0454\n",
      "Epoch [1/10], Step [103/124], Loss: 0.0639\n",
      "Epoch [1/10], Step [104/124], Loss: 0.0668\n",
      "Epoch [1/10], Step [105/124], Loss: 0.0588\n",
      "Epoch [1/10], Step [106/124], Loss: 0.0723\n",
      "Epoch [1/10], Step [107/124], Loss: 0.0374\n",
      "Epoch [1/10], Step [108/124], Loss: 0.0443\n",
      "Epoch [1/10], Step [109/124], Loss: 0.0244\n",
      "Epoch [1/10], Step [110/124], Loss: 0.0593\n",
      "Epoch [1/10], Step [111/124], Loss: 0.0655\n",
      "Epoch [1/10], Step [112/124], Loss: 0.0424\n",
      "Epoch [1/10], Step [113/124], Loss: 0.0457\n",
      "Epoch [1/10], Step [114/124], Loss: 0.0234\n",
      "Epoch [1/10], Step [115/124], Loss: 0.0208\n",
      "Epoch [1/10], Step [116/124], Loss: 0.0596\n",
      "Epoch [1/10], Step [117/124], Loss: 0.0703\n",
      "Epoch [1/10], Step [118/124], Loss: 0.0578\n",
      "Epoch [1/10], Step [119/124], Loss: 0.0699\n",
      "Epoch [1/10], Step [120/124], Loss: 0.0332\n",
      "Epoch [1/10], Step [121/124], Loss: 0.0304\n",
      "Epoch [1/10], Step [122/124], Loss: 0.0780\n",
      "Epoch [1/10], Step [123/124], Loss: 0.0310\n",
      "Epoch [1/10], Step [124/124], Loss: 0.0720\n",
      "Epoch [2/10], Step [1/124], Loss: 0.0480\n",
      "Epoch [2/10], Step [2/124], Loss: 0.0603\n",
      "Epoch [2/10], Step [3/124], Loss: 0.0417\n",
      "Epoch [2/10], Step [4/124], Loss: 0.0616\n",
      "Epoch [2/10], Step [5/124], Loss: 0.0456\n",
      "Epoch [2/10], Step [6/124], Loss: 0.0575\n",
      "Epoch [2/10], Step [7/124], Loss: 0.0746\n",
      "Epoch [2/10], Step [8/124], Loss: 0.0776\n",
      "Epoch [2/10], Step [9/124], Loss: 0.0684\n",
      "Epoch [2/10], Step [10/124], Loss: 0.0666\n",
      "Epoch [2/10], Step [11/124], Loss: 0.0349\n",
      "Epoch [2/10], Step [12/124], Loss: 0.0519\n",
      "Epoch [2/10], Step [13/124], Loss: 0.0450\n",
      "Epoch [2/10], Step [14/124], Loss: 0.0576\n",
      "Epoch [2/10], Step [15/124], Loss: 0.0379\n",
      "Epoch [2/10], Step [16/124], Loss: 0.0444\n",
      "Epoch [2/10], Step [17/124], Loss: 0.0240\n",
      "Epoch [2/10], Step [18/124], Loss: 0.0494\n",
      "Epoch [2/10], Step [19/124], Loss: 0.0308\n",
      "Epoch [2/10], Step [20/124], Loss: 0.0290\n",
      "Epoch [2/10], Step [21/124], Loss: 0.0248\n",
      "Epoch [2/10], Step [22/124], Loss: 0.0602\n",
      "Epoch [2/10], Step [23/124], Loss: 0.0357\n",
      "Epoch [2/10], Step [24/124], Loss: 0.0179\n",
      "Epoch [2/10], Step [25/124], Loss: 0.0103\n",
      "Epoch [2/10], Step [26/124], Loss: 0.0748\n",
      "Epoch [2/10], Step [27/124], Loss: 0.0728\n",
      "Epoch [2/10], Step [28/124], Loss: 0.0186\n",
      "Epoch [2/10], Step [29/124], Loss: 0.0267\n",
      "Epoch [2/10], Step [30/124], Loss: 0.0192\n",
      "Epoch [2/10], Step [31/124], Loss: 0.0200\n",
      "Epoch [2/10], Step [32/124], Loss: 0.0436\n",
      "Epoch [2/10], Step [33/124], Loss: 0.0391\n",
      "Epoch [2/10], Step [34/124], Loss: 0.0181\n",
      "Epoch [2/10], Step [35/124], Loss: 0.0811\n",
      "Epoch [2/10], Step [36/124], Loss: 0.0257\n",
      "Epoch [2/10], Step [37/124], Loss: 0.0693\n",
      "Epoch [2/10], Step [38/124], Loss: 0.0321\n",
      "Epoch [2/10], Step [39/124], Loss: 0.0225\n",
      "Epoch [2/10], Step [40/124], Loss: 0.0379\n",
      "Epoch [2/10], Step [41/124], Loss: 0.0237\n",
      "Epoch [2/10], Step [42/124], Loss: 0.1061\n",
      "Epoch [2/10], Step [43/124], Loss: 0.0411\n",
      "Epoch [2/10], Step [44/124], Loss: 0.0362\n",
      "Epoch [2/10], Step [45/124], Loss: 0.0299\n",
      "Epoch [2/10], Step [46/124], Loss: 0.0391\n",
      "Epoch [2/10], Step [47/124], Loss: 0.0477\n",
      "Epoch [2/10], Step [48/124], Loss: 0.0383\n",
      "Epoch [2/10], Step [49/124], Loss: 0.0243\n",
      "Epoch [2/10], Step [50/124], Loss: 0.0253\n",
      "Epoch [2/10], Step [51/124], Loss: 0.0118\n",
      "Epoch [2/10], Step [52/124], Loss: 0.0699\n",
      "Epoch [2/10], Step [53/124], Loss: 0.0245\n",
      "Epoch [2/10], Step [54/124], Loss: 0.0781\n",
      "Epoch [2/10], Step [55/124], Loss: 0.0340\n",
      "Epoch [2/10], Step [56/124], Loss: 0.0776\n",
      "Epoch [2/10], Step [57/124], Loss: 0.0202\n",
      "Epoch [2/10], Step [58/124], Loss: 0.0612\n",
      "Epoch [2/10], Step [59/124], Loss: 0.0355\n",
      "Epoch [2/10], Step [60/124], Loss: 0.0385\n",
      "Epoch [2/10], Step [61/124], Loss: 0.0619\n",
      "Epoch [2/10], Step [62/124], Loss: 0.0877\n",
      "Epoch [2/10], Step [63/124], Loss: 0.0313\n",
      "Epoch [2/10], Step [64/124], Loss: 0.0230\n",
      "Epoch [2/10], Step [65/124], Loss: 0.0322\n",
      "Epoch [2/10], Step [66/124], Loss: 0.0473\n",
      "Epoch [2/10], Step [67/124], Loss: 0.0502\n",
      "Epoch [2/10], Step [68/124], Loss: 0.0908\n",
      "Epoch [2/10], Step [69/124], Loss: 0.0739\n",
      "Epoch [2/10], Step [70/124], Loss: 0.0327\n",
      "Epoch [2/10], Step [71/124], Loss: 0.0723\n",
      "Epoch [2/10], Step [72/124], Loss: 0.0373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [73/124], Loss: 0.0495\n",
      "Epoch [2/10], Step [74/124], Loss: 0.0420\n",
      "Epoch [2/10], Step [75/124], Loss: 0.0232\n",
      "Epoch [2/10], Step [76/124], Loss: 0.0640\n",
      "Epoch [2/10], Step [77/124], Loss: 0.0653\n",
      "Epoch [2/10], Step [78/124], Loss: 0.0363\n",
      "Epoch [2/10], Step [79/124], Loss: 0.0778\n",
      "Epoch [2/10], Step [80/124], Loss: 0.0368\n",
      "Epoch [2/10], Step [81/124], Loss: 0.0317\n",
      "Epoch [2/10], Step [82/124], Loss: 0.0845\n",
      "Epoch [2/10], Step [83/124], Loss: 0.0484\n",
      "Epoch [2/10], Step [84/124], Loss: 0.0230\n",
      "Epoch [2/10], Step [85/124], Loss: 0.0412\n",
      "Epoch [2/10], Step [86/124], Loss: 0.0487\n",
      "Epoch [2/10], Step [87/124], Loss: 0.0556\n",
      "Epoch [2/10], Step [88/124], Loss: 0.0119\n",
      "Epoch [2/10], Step [89/124], Loss: 0.0501\n",
      "Epoch [2/10], Step [90/124], Loss: 0.0353\n",
      "Epoch [2/10], Step [91/124], Loss: 0.0609\n",
      "Epoch [2/10], Step [92/124], Loss: 0.0263\n",
      "Epoch [2/10], Step [93/124], Loss: 0.0691\n",
      "Epoch [2/10], Step [94/124], Loss: 0.0819\n",
      "Epoch [2/10], Step [95/124], Loss: 0.0393\n",
      "Epoch [2/10], Step [96/124], Loss: 0.0369\n",
      "Epoch [2/10], Step [97/124], Loss: 0.1065\n",
      "Epoch [2/10], Step [98/124], Loss: 0.0278\n",
      "Epoch [2/10], Step [99/124], Loss: 0.0405\n",
      "Epoch [2/10], Step [100/124], Loss: 0.0299\n",
      "Epoch [2/10], Step [101/124], Loss: 0.0336\n",
      "Epoch [2/10], Step [102/124], Loss: 0.0897\n",
      "Epoch [2/10], Step [103/124], Loss: 0.0380\n",
      "Epoch [2/10], Step [104/124], Loss: 0.0599\n",
      "Epoch [2/10], Step [105/124], Loss: 0.0661\n",
      "Epoch [2/10], Step [106/124], Loss: 0.0154\n",
      "Epoch [2/10], Step [107/124], Loss: 0.0920\n",
      "Epoch [2/10], Step [108/124], Loss: 0.0732\n",
      "Epoch [2/10], Step [109/124], Loss: 0.0585\n",
      "Epoch [2/10], Step [110/124], Loss: 0.0593\n",
      "Epoch [2/10], Step [111/124], Loss: 0.0518\n",
      "Epoch [2/10], Step [112/124], Loss: 0.0651\n",
      "Epoch [2/10], Step [113/124], Loss: 0.0249\n",
      "Epoch [2/10], Step [114/124], Loss: 0.0227\n",
      "Epoch [2/10], Step [115/124], Loss: 0.0546\n",
      "Epoch [2/10], Step [116/124], Loss: 0.0378\n",
      "Epoch [2/10], Step [117/124], Loss: 0.0269\n",
      "Epoch [2/10], Step [118/124], Loss: 0.0693\n",
      "Epoch [2/10], Step [119/124], Loss: 0.0675\n",
      "Epoch [2/10], Step [120/124], Loss: 0.0493\n",
      "Epoch [2/10], Step [121/124], Loss: 0.0436\n",
      "Epoch [2/10], Step [122/124], Loss: 0.0236\n",
      "Epoch [2/10], Step [123/124], Loss: 0.0603\n",
      "Epoch [2/10], Step [124/124], Loss: 0.0051\n",
      "Epoch [3/10], Step [1/124], Loss: 0.0574\n",
      "Epoch [3/10], Step [2/124], Loss: 0.0554\n",
      "Epoch [3/10], Step [3/124], Loss: 0.0432\n",
      "Epoch [3/10], Step [4/124], Loss: 0.0479\n",
      "Epoch [3/10], Step [5/124], Loss: 0.0349\n",
      "Epoch [3/10], Step [6/124], Loss: 0.0628\n",
      "Epoch [3/10], Step [7/124], Loss: 0.0302\n",
      "Epoch [3/10], Step [8/124], Loss: 0.0522\n",
      "Epoch [3/10], Step [9/124], Loss: 0.0379\n",
      "Epoch [3/10], Step [10/124], Loss: 0.0202\n",
      "Epoch [3/10], Step [11/124], Loss: 0.0197\n",
      "Epoch [3/10], Step [12/124], Loss: 0.0441\n",
      "Epoch [3/10], Step [13/124], Loss: 0.0334\n",
      "Epoch [3/10], Step [14/124], Loss: 0.0605\n",
      "Epoch [3/10], Step [15/124], Loss: 0.0344\n",
      "Epoch [3/10], Step [16/124], Loss: 0.0287\n",
      "Epoch [3/10], Step [17/124], Loss: 0.0220\n",
      "Epoch [3/10], Step [18/124], Loss: 0.0118\n",
      "Epoch [3/10], Step [19/124], Loss: 0.0641\n",
      "Epoch [3/10], Step [20/124], Loss: 0.0486\n",
      "Epoch [3/10], Step [21/124], Loss: 0.0467\n",
      "Epoch [3/10], Step [22/124], Loss: 0.0535\n",
      "Epoch [3/10], Step [23/124], Loss: 0.0314\n",
      "Epoch [3/10], Step [24/124], Loss: 0.0113\n",
      "Epoch [3/10], Step [25/124], Loss: 0.0344\n",
      "Epoch [3/10], Step [26/124], Loss: 0.0172\n",
      "Epoch [3/10], Step [27/124], Loss: 0.0284\n",
      "Epoch [3/10], Step [28/124], Loss: 0.0354\n",
      "Epoch [3/10], Step [29/124], Loss: 0.0524\n",
      "Epoch [3/10], Step [30/124], Loss: 0.0333\n",
      "Epoch [3/10], Step [31/124], Loss: 0.0152\n",
      "Epoch [3/10], Step [32/124], Loss: 0.0425\n",
      "Epoch [3/10], Step [33/124], Loss: 0.0388\n",
      "Epoch [3/10], Step [34/124], Loss: 0.0516\n",
      "Epoch [3/10], Step [35/124], Loss: 0.0446\n",
      "Epoch [3/10], Step [36/124], Loss: 0.0242\n",
      "Epoch [3/10], Step [37/124], Loss: 0.0624\n",
      "Epoch [3/10], Step [38/124], Loss: 0.0278\n",
      "Epoch [3/10], Step [39/124], Loss: 0.0313\n",
      "Epoch [3/10], Step [40/124], Loss: 0.0370\n",
      "Epoch [3/10], Step [41/124], Loss: 0.0235\n",
      "Epoch [3/10], Step [42/124], Loss: 0.0453\n",
      "Epoch [3/10], Step [43/124], Loss: 0.0390\n",
      "Epoch [3/10], Step [44/124], Loss: 0.0229\n",
      "Epoch [3/10], Step [45/124], Loss: 0.0130\n",
      "Epoch [3/10], Step [46/124], Loss: 0.0578\n",
      "Epoch [3/10], Step [47/124], Loss: 0.0562\n",
      "Epoch [3/10], Step [48/124], Loss: 0.0372\n",
      "Epoch [3/10], Step [49/124], Loss: 0.0254\n",
      "Epoch [3/10], Step [50/124], Loss: 0.0333\n",
      "Epoch [3/10], Step [51/124], Loss: 0.0176\n",
      "Epoch [3/10], Step [52/124], Loss: 0.0569\n",
      "Epoch [3/10], Step [53/124], Loss: 0.0837\n",
      "Epoch [3/10], Step [54/124], Loss: 0.0234\n",
      "Epoch [3/10], Step [55/124], Loss: 0.0351\n",
      "Epoch [3/10], Step [56/124], Loss: 0.0803\n",
      "Epoch [3/10], Step [57/124], Loss: 0.0388\n",
      "Epoch [3/10], Step [58/124], Loss: 0.0696\n",
      "Epoch [3/10], Step [59/124], Loss: 0.0460\n",
      "Epoch [3/10], Step [60/124], Loss: 0.1194\n",
      "Epoch [3/10], Step [61/124], Loss: 0.0676\n",
      "Epoch [3/10], Step [62/124], Loss: 0.0292\n",
      "Epoch [3/10], Step [63/124], Loss: 0.0518\n",
      "Epoch [3/10], Step [64/124], Loss: 0.0365\n",
      "Epoch [3/10], Step [65/124], Loss: 0.0554\n",
      "Epoch [3/10], Step [66/124], Loss: 0.0473\n",
      "Epoch [3/10], Step [67/124], Loss: 0.0249\n",
      "Epoch [3/10], Step [68/124], Loss: 0.0268\n",
      "Epoch [3/10], Step [69/124], Loss: 0.0237\n",
      "Epoch [3/10], Step [70/124], Loss: 0.0386\n",
      "Epoch [3/10], Step [71/124], Loss: 0.0776\n",
      "Epoch [3/10], Step [72/124], Loss: 0.0603\n",
      "Epoch [3/10], Step [73/124], Loss: 0.0223\n",
      "Epoch [3/10], Step [74/124], Loss: 0.0278\n",
      "Epoch [3/10], Step [75/124], Loss: 0.0203\n",
      "Epoch [3/10], Step [76/124], Loss: 0.0217\n",
      "Epoch [3/10], Step [77/124], Loss: 0.0246\n",
      "Epoch [3/10], Step [78/124], Loss: 0.0446\n",
      "Epoch [3/10], Step [79/124], Loss: 0.0311\n",
      "Epoch [3/10], Step [80/124], Loss: 0.0401\n",
      "Epoch [3/10], Step [81/124], Loss: 0.0533\n",
      "Epoch [3/10], Step [82/124], Loss: 0.0320\n",
      "Epoch [3/10], Step [83/124], Loss: 0.0737\n",
      "Epoch [3/10], Step [84/124], Loss: 0.0592\n",
      "Epoch [3/10], Step [85/124], Loss: 0.0090\n",
      "Epoch [3/10], Step [86/124], Loss: 0.0754\n",
      "Epoch [3/10], Step [87/124], Loss: 0.0370\n",
      "Epoch [3/10], Step [88/124], Loss: 0.0575\n",
      "Epoch [3/10], Step [89/124], Loss: 0.0486\n",
      "Epoch [3/10], Step [90/124], Loss: 0.0441\n",
      "Epoch [3/10], Step [91/124], Loss: 0.0676\n",
      "Epoch [3/10], Step [92/124], Loss: 0.0738\n",
      "Epoch [3/10], Step [93/124], Loss: 0.0183\n",
      "Epoch [3/10], Step [94/124], Loss: 0.0401\n",
      "Epoch [3/10], Step [95/124], Loss: 0.0422\n",
      "Epoch [3/10], Step [96/124], Loss: 0.0537\n",
      "Epoch [3/10], Step [97/124], Loss: 0.0099\n",
      "Epoch [3/10], Step [98/124], Loss: 0.0524\n",
      "Epoch [3/10], Step [99/124], Loss: 0.0258\n",
      "Epoch [3/10], Step [100/124], Loss: 0.0378\n",
      "Epoch [3/10], Step [101/124], Loss: 0.0287\n",
      "Epoch [3/10], Step [102/124], Loss: 0.0476\n",
      "Epoch [3/10], Step [103/124], Loss: 0.0235\n",
      "Epoch [3/10], Step [104/124], Loss: 0.0191\n",
      "Epoch [3/10], Step [105/124], Loss: 0.0707\n",
      "Epoch [3/10], Step [106/124], Loss: 0.0245\n",
      "Epoch [3/10], Step [107/124], Loss: 0.0349\n",
      "Epoch [3/10], Step [108/124], Loss: 0.0299\n",
      "Epoch [3/10], Step [109/124], Loss: 0.0532\n",
      "Epoch [3/10], Step [110/124], Loss: 0.0602\n",
      "Epoch [3/10], Step [111/124], Loss: 0.0505\n",
      "Epoch [3/10], Step [112/124], Loss: 0.0292\n",
      "Epoch [3/10], Step [113/124], Loss: 0.0237\n",
      "Epoch [3/10], Step [114/124], Loss: 0.0436\n",
      "Epoch [3/10], Step [115/124], Loss: 0.0328\n",
      "Epoch [3/10], Step [116/124], Loss: 0.0314\n",
      "Epoch [3/10], Step [117/124], Loss: 0.0189\n",
      "Epoch [3/10], Step [118/124], Loss: 0.0436\n",
      "Epoch [3/10], Step [119/124], Loss: 0.0128\n",
      "Epoch [3/10], Step [120/124], Loss: 0.0263\n",
      "Epoch [3/10], Step [121/124], Loss: 0.0193\n",
      "Epoch [3/10], Step [122/124], Loss: 0.0365\n",
      "Epoch [3/10], Step [123/124], Loss: 0.0372\n",
      "Epoch [3/10], Step [124/124], Loss: 0.0013\n",
      "Epoch [4/10], Step [1/124], Loss: 0.0196\n",
      "Epoch [4/10], Step [2/124], Loss: 0.0518\n",
      "Epoch [4/10], Step [3/124], Loss: 0.0247\n",
      "Epoch [4/10], Step [4/124], Loss: 0.0321\n",
      "Epoch [4/10], Step [5/124], Loss: 0.0463\n",
      "Epoch [4/10], Step [6/124], Loss: 0.0192\n",
      "Epoch [4/10], Step [7/124], Loss: 0.0114\n",
      "Epoch [4/10], Step [8/124], Loss: 0.0535\n",
      "Epoch [4/10], Step [9/124], Loss: 0.0239\n",
      "Epoch [4/10], Step [10/124], Loss: 0.0096\n",
      "Epoch [4/10], Step [11/124], Loss: 0.0541\n",
      "Epoch [4/10], Step [12/124], Loss: 0.0395\n",
      "Epoch [4/10], Step [13/124], Loss: 0.0417\n",
      "Epoch [4/10], Step [14/124], Loss: 0.0448\n",
      "Epoch [4/10], Step [15/124], Loss: 0.0461\n",
      "Epoch [4/10], Step [16/124], Loss: 0.0328\n",
      "Epoch [4/10], Step [17/124], Loss: 0.0473\n",
      "Epoch [4/10], Step [18/124], Loss: 0.0394\n",
      "Epoch [4/10], Step [19/124], Loss: 0.0257\n",
      "Epoch [4/10], Step [20/124], Loss: 0.0298\n",
      "Epoch [4/10], Step [21/124], Loss: 0.0431\n",
      "Epoch [4/10], Step [22/124], Loss: 0.0101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [23/124], Loss: 0.0507\n",
      "Epoch [4/10], Step [24/124], Loss: 0.0305\n",
      "Epoch [4/10], Step [25/124], Loss: 0.0175\n",
      "Epoch [4/10], Step [26/124], Loss: 0.0409\n",
      "Epoch [4/10], Step [27/124], Loss: 0.0263\n",
      "Epoch [4/10], Step [28/124], Loss: 0.0152\n",
      "Epoch [4/10], Step [29/124], Loss: 0.0363\n",
      "Epoch [4/10], Step [30/124], Loss: 0.0209\n",
      "Epoch [4/10], Step [31/124], Loss: 0.0587\n",
      "Epoch [4/10], Step [32/124], Loss: 0.0534\n",
      "Epoch [4/10], Step [33/124], Loss: 0.0098\n",
      "Epoch [4/10], Step [34/124], Loss: 0.0443\n",
      "Epoch [4/10], Step [35/124], Loss: 0.0197\n",
      "Epoch [4/10], Step [36/124], Loss: 0.0497\n",
      "Epoch [4/10], Step [37/124], Loss: 0.0170\n",
      "Epoch [4/10], Step [38/124], Loss: 0.0358\n",
      "Epoch [4/10], Step [39/124], Loss: 0.0331\n",
      "Epoch [4/10], Step [40/124], Loss: 0.0592\n",
      "Epoch [4/10], Step [41/124], Loss: 0.0353\n",
      "Epoch [4/10], Step [42/124], Loss: 0.0303\n",
      "Epoch [4/10], Step [43/124], Loss: 0.0339\n",
      "Epoch [4/10], Step [44/124], Loss: 0.0612\n",
      "Epoch [4/10], Step [45/124], Loss: 0.0225\n",
      "Epoch [4/10], Step [46/124], Loss: 0.0286\n",
      "Epoch [4/10], Step [47/124], Loss: 0.0137\n",
      "Epoch [4/10], Step [48/124], Loss: 0.1338\n",
      "Epoch [4/10], Step [49/124], Loss: 0.0252\n",
      "Epoch [4/10], Step [50/124], Loss: 0.0430\n",
      "Epoch [4/10], Step [51/124], Loss: 0.0592\n",
      "Epoch [4/10], Step [52/124], Loss: 0.0341\n",
      "Epoch [4/10], Step [53/124], Loss: 0.0334\n",
      "Epoch [4/10], Step [54/124], Loss: 0.0291\n",
      "Epoch [4/10], Step [55/124], Loss: 0.0436\n",
      "Epoch [4/10], Step [56/124], Loss: 0.0274\n",
      "Epoch [4/10], Step [57/124], Loss: 0.0188\n",
      "Epoch [4/10], Step [58/124], Loss: 0.0680\n",
      "Epoch [4/10], Step [59/124], Loss: 0.0182\n",
      "Epoch [4/10], Step [60/124], Loss: 0.0221\n",
      "Epoch [4/10], Step [61/124], Loss: 0.0341\n",
      "Epoch [4/10], Step [62/124], Loss: 0.0218\n",
      "Epoch [4/10], Step [63/124], Loss: 0.0320\n",
      "Epoch [4/10], Step [64/124], Loss: 0.0421\n",
      "Epoch [4/10], Step [65/124], Loss: 0.0547\n",
      "Epoch [4/10], Step [66/124], Loss: 0.0179\n",
      "Epoch [4/10], Step [67/124], Loss: 0.0181\n",
      "Epoch [4/10], Step [68/124], Loss: 0.0443\n",
      "Epoch [4/10], Step [69/124], Loss: 0.0424\n",
      "Epoch [4/10], Step [70/124], Loss: 0.0284\n",
      "Epoch [4/10], Step [71/124], Loss: 0.0326\n",
      "Epoch [4/10], Step [72/124], Loss: 0.0381\n",
      "Epoch [4/10], Step [73/124], Loss: 0.0273\n",
      "Epoch [4/10], Step [74/124], Loss: 0.0160\n",
      "Epoch [4/10], Step [75/124], Loss: 0.0223\n",
      "Epoch [4/10], Step [76/124], Loss: 0.0561\n",
      "Epoch [4/10], Step [77/124], Loss: 0.0524\n",
      "Epoch [4/10], Step [78/124], Loss: 0.0492\n",
      "Epoch [4/10], Step [79/124], Loss: 0.0299\n",
      "Epoch [4/10], Step [80/124], Loss: 0.0486\n",
      "Epoch [4/10], Step [81/124], Loss: 0.0271\n",
      "Epoch [4/10], Step [82/124], Loss: 0.0411\n",
      "Epoch [4/10], Step [83/124], Loss: 0.0482\n",
      "Epoch [4/10], Step [84/124], Loss: 0.0461\n",
      "Epoch [4/10], Step [85/124], Loss: 0.0294\n",
      "Epoch [4/10], Step [86/124], Loss: 0.0411\n",
      "Epoch [4/10], Step [87/124], Loss: 0.0281\n",
      "Epoch [4/10], Step [88/124], Loss: 0.0217\n",
      "Epoch [4/10], Step [89/124], Loss: 0.0159\n",
      "Epoch [4/10], Step [90/124], Loss: 0.0310\n",
      "Epoch [4/10], Step [91/124], Loss: 0.0187\n",
      "Epoch [4/10], Step [92/124], Loss: 0.0367\n",
      "Epoch [4/10], Step [93/124], Loss: 0.0184\n",
      "Epoch [4/10], Step [94/124], Loss: 0.0317\n",
      "Epoch [4/10], Step [95/124], Loss: 0.0432\n",
      "Epoch [4/10], Step [96/124], Loss: 0.0889\n",
      "Epoch [4/10], Step [97/124], Loss: 0.0196\n",
      "Epoch [4/10], Step [98/124], Loss: 0.0275\n",
      "Epoch [4/10], Step [99/124], Loss: 0.0152\n",
      "Epoch [4/10], Step [100/124], Loss: 0.0466\n",
      "Epoch [4/10], Step [101/124], Loss: 0.0549\n",
      "Epoch [4/10], Step [102/124], Loss: 0.0188\n",
      "Epoch [4/10], Step [103/124], Loss: 0.0312\n",
      "Epoch [4/10], Step [104/124], Loss: 0.0409\n",
      "Epoch [4/10], Step [105/124], Loss: 0.0425\n",
      "Epoch [4/10], Step [106/124], Loss: 0.0316\n",
      "Epoch [4/10], Step [107/124], Loss: 0.0253\n",
      "Epoch [4/10], Step [108/124], Loss: 0.0290\n",
      "Epoch [4/10], Step [109/124], Loss: 0.0385\n",
      "Epoch [4/10], Step [110/124], Loss: 0.0332\n",
      "Epoch [4/10], Step [111/124], Loss: 0.0566\n",
      "Epoch [4/10], Step [112/124], Loss: 0.0482\n",
      "Epoch [4/10], Step [113/124], Loss: 0.0326\n",
      "Epoch [4/10], Step [114/124], Loss: 0.0161\n",
      "Epoch [4/10], Step [115/124], Loss: 0.0390\n",
      "Epoch [4/10], Step [116/124], Loss: 0.0393\n",
      "Epoch [4/10], Step [117/124], Loss: 0.0597\n",
      "Epoch [4/10], Step [118/124], Loss: 0.0177\n",
      "Epoch [4/10], Step [119/124], Loss: 0.0522\n",
      "Epoch [4/10], Step [120/124], Loss: 0.0214\n",
      "Epoch [4/10], Step [121/124], Loss: 0.0264\n",
      "Epoch [4/10], Step [122/124], Loss: 0.0235\n",
      "Epoch [4/10], Step [123/124], Loss: 0.0237\n",
      "Epoch [4/10], Step [124/124], Loss: 0.0003\n",
      "Epoch [5/10], Step [1/124], Loss: 0.0165\n",
      "Epoch [5/10], Step [2/124], Loss: 0.0748\n",
      "Epoch [5/10], Step [3/124], Loss: 0.0455\n",
      "Epoch [5/10], Step [4/124], Loss: 0.0105\n",
      "Epoch [5/10], Step [5/124], Loss: 0.0212\n",
      "Epoch [5/10], Step [6/124], Loss: 0.0373\n",
      "Epoch [5/10], Step [7/124], Loss: 0.0174\n",
      "Epoch [5/10], Step [8/124], Loss: 0.0212\n",
      "Epoch [5/10], Step [9/124], Loss: 0.0845\n",
      "Epoch [5/10], Step [10/124], Loss: 0.0316\n",
      "Epoch [5/10], Step [11/124], Loss: 0.0198\n",
      "Epoch [5/10], Step [12/124], Loss: 0.0111\n",
      "Epoch [5/10], Step [13/124], Loss: 0.0287\n",
      "Epoch [5/10], Step [14/124], Loss: 0.0292\n",
      "Epoch [5/10], Step [15/124], Loss: 0.0560\n",
      "Epoch [5/10], Step [16/124], Loss: 0.0381\n",
      "Epoch [5/10], Step [17/124], Loss: 0.0249\n",
      "Epoch [5/10], Step [18/124], Loss: 0.0315\n",
      "Epoch [5/10], Step [19/124], Loss: 0.0294\n",
      "Epoch [5/10], Step [20/124], Loss: 0.0165\n",
      "Epoch [5/10], Step [21/124], Loss: 0.0166\n",
      "Epoch [5/10], Step [22/124], Loss: 0.0360\n",
      "Epoch [5/10], Step [23/124], Loss: 0.0233\n",
      "Epoch [5/10], Step [24/124], Loss: 0.0355\n",
      "Epoch [5/10], Step [25/124], Loss: 0.0474\n",
      "Epoch [5/10], Step [26/124], Loss: 0.0284\n",
      "Epoch [5/10], Step [27/124], Loss: 0.0457\n",
      "Epoch [5/10], Step [28/124], Loss: 0.0295\n",
      "Epoch [5/10], Step [29/124], Loss: 0.0317\n",
      "Epoch [5/10], Step [30/124], Loss: 0.0402\n",
      "Epoch [5/10], Step [31/124], Loss: 0.0152\n",
      "Epoch [5/10], Step [32/124], Loss: 0.0234\n",
      "Epoch [5/10], Step [33/124], Loss: 0.0186\n",
      "Epoch [5/10], Step [34/124], Loss: 0.0230\n",
      "Epoch [5/10], Step [35/124], Loss: 0.0334\n",
      "Epoch [5/10], Step [36/124], Loss: 0.0228\n",
      "Epoch [5/10], Step [37/124], Loss: 0.0138\n",
      "Epoch [5/10], Step [38/124], Loss: 0.0131\n",
      "Epoch [5/10], Step [39/124], Loss: 0.0422\n",
      "Epoch [5/10], Step [40/124], Loss: 0.0513\n",
      "Epoch [5/10], Step [41/124], Loss: 0.0238\n",
      "Epoch [5/10], Step [42/124], Loss: 0.0108\n",
      "Epoch [5/10], Step [43/124], Loss: 0.0344\n",
      "Epoch [5/10], Step [44/124], Loss: 0.0137\n",
      "Epoch [5/10], Step [45/124], Loss: 0.0247\n",
      "Epoch [5/10], Step [46/124], Loss: 0.0216\n",
      "Epoch [5/10], Step [47/124], Loss: 0.0487\n",
      "Epoch [5/10], Step [48/124], Loss: 0.0324\n",
      "Epoch [5/10], Step [49/124], Loss: 0.0358\n",
      "Epoch [5/10], Step [50/124], Loss: 0.0542\n",
      "Epoch [5/10], Step [51/124], Loss: 0.0788\n",
      "Epoch [5/10], Step [52/124], Loss: 0.0070\n",
      "Epoch [5/10], Step [53/124], Loss: 0.0360\n",
      "Epoch [5/10], Step [54/124], Loss: 0.0257\n",
      "Epoch [5/10], Step [55/124], Loss: 0.0427\n",
      "Epoch [5/10], Step [56/124], Loss: 0.0175\n",
      "Epoch [5/10], Step [57/124], Loss: 0.0254\n",
      "Epoch [5/10], Step [58/124], Loss: 0.0316\n",
      "Epoch [5/10], Step [59/124], Loss: 0.0352\n",
      "Epoch [5/10], Step [60/124], Loss: 0.0321\n",
      "Epoch [5/10], Step [61/124], Loss: 0.0213\n",
      "Epoch [5/10], Step [62/124], Loss: 0.0304\n",
      "Epoch [5/10], Step [63/124], Loss: 0.0547\n",
      "Epoch [5/10], Step [64/124], Loss: 0.0657\n",
      "Epoch [5/10], Step [65/124], Loss: 0.0077\n",
      "Epoch [5/10], Step [66/124], Loss: 0.0543\n",
      "Epoch [5/10], Step [67/124], Loss: 0.0205\n",
      "Epoch [5/10], Step [68/124], Loss: 0.0233\n",
      "Epoch [5/10], Step [69/124], Loss: 0.0430\n",
      "Epoch [5/10], Step [70/124], Loss: 0.0250\n",
      "Epoch [5/10], Step [71/124], Loss: 0.0159\n",
      "Epoch [5/10], Step [72/124], Loss: 0.0226\n",
      "Epoch [5/10], Step [73/124], Loss: 0.0157\n",
      "Epoch [5/10], Step [74/124], Loss: 0.0389\n",
      "Epoch [5/10], Step [75/124], Loss: 0.0260\n",
      "Epoch [5/10], Step [76/124], Loss: 0.0294\n",
      "Epoch [5/10], Step [77/124], Loss: 0.0177\n",
      "Epoch [5/10], Step [78/124], Loss: 0.0402\n",
      "Epoch [5/10], Step [79/124], Loss: 0.0408\n",
      "Epoch [5/10], Step [80/124], Loss: 0.0213\n",
      "Epoch [5/10], Step [81/124], Loss: 0.0104\n",
      "Epoch [5/10], Step [82/124], Loss: 0.0306\n",
      "Epoch [5/10], Step [83/124], Loss: 0.0231\n",
      "Epoch [5/10], Step [84/124], Loss: 0.0195\n",
      "Epoch [5/10], Step [85/124], Loss: 0.0534\n",
      "Epoch [5/10], Step [86/124], Loss: 0.0383\n",
      "Epoch [5/10], Step [87/124], Loss: 0.0289\n",
      "Epoch [5/10], Step [88/124], Loss: 0.0454\n",
      "Epoch [5/10], Step [89/124], Loss: 0.0325\n",
      "Epoch [5/10], Step [90/124], Loss: 0.0120\n",
      "Epoch [5/10], Step [91/124], Loss: 0.0427\n",
      "Epoch [5/10], Step [92/124], Loss: 0.0225\n",
      "Epoch [5/10], Step [93/124], Loss: 0.0106\n",
      "Epoch [5/10], Step [94/124], Loss: 0.0329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [95/124], Loss: 0.0234\n",
      "Epoch [5/10], Step [96/124], Loss: 0.0278\n",
      "Epoch [5/10], Step [97/124], Loss: 0.0156\n",
      "Epoch [5/10], Step [98/124], Loss: 0.0274\n",
      "Epoch [5/10], Step [99/124], Loss: 0.0378\n",
      "Epoch [5/10], Step [100/124], Loss: 0.0533\n",
      "Epoch [5/10], Step [101/124], Loss: 0.0087\n",
      "Epoch [5/10], Step [102/124], Loss: 0.0243\n",
      "Epoch [5/10], Step [103/124], Loss: 0.0753\n",
      "Epoch [5/10], Step [104/124], Loss: 0.0298\n",
      "Epoch [5/10], Step [105/124], Loss: 0.0338\n",
      "Epoch [5/10], Step [106/124], Loss: 0.0196\n",
      "Epoch [5/10], Step [107/124], Loss: 0.0141\n",
      "Epoch [5/10], Step [108/124], Loss: 0.0257\n",
      "Epoch [5/10], Step [109/124], Loss: 0.0306\n",
      "Epoch [5/10], Step [110/124], Loss: 0.0756\n",
      "Epoch [5/10], Step [111/124], Loss: 0.0275\n",
      "Epoch [5/10], Step [112/124], Loss: 0.0402\n",
      "Epoch [5/10], Step [113/124], Loss: 0.0424\n",
      "Epoch [5/10], Step [114/124], Loss: 0.0321\n",
      "Epoch [5/10], Step [115/124], Loss: 0.0237\n",
      "Epoch [5/10], Step [116/124], Loss: 0.0467\n",
      "Epoch [5/10], Step [117/124], Loss: 0.0064\n",
      "Epoch [5/10], Step [118/124], Loss: 0.0211\n",
      "Epoch [5/10], Step [119/124], Loss: 0.0804\n",
      "Epoch [5/10], Step [120/124], Loss: 0.0235\n",
      "Epoch [5/10], Step [121/124], Loss: 0.0226\n",
      "Epoch [5/10], Step [122/124], Loss: 0.0337\n",
      "Epoch [5/10], Step [123/124], Loss: 0.0217\n",
      "Epoch [5/10], Step [124/124], Loss: 0.0020\n",
      "Epoch [6/10], Step [1/124], Loss: 0.0566\n",
      "Epoch [6/10], Step [2/124], Loss: 0.0268\n",
      "Epoch [6/10], Step [3/124], Loss: 0.0243\n",
      "Epoch [6/10], Step [4/124], Loss: 0.0473\n",
      "Epoch [6/10], Step [5/124], Loss: 0.0334\n",
      "Epoch [6/10], Step [6/124], Loss: 0.0159\n",
      "Epoch [6/10], Step [7/124], Loss: 0.0167\n",
      "Epoch [6/10], Step [8/124], Loss: 0.0457\n",
      "Epoch [6/10], Step [9/124], Loss: 0.0337\n",
      "Epoch [6/10], Step [10/124], Loss: 0.0332\n",
      "Epoch [6/10], Step [11/124], Loss: 0.0672\n",
      "Epoch [6/10], Step [12/124], Loss: 0.0200\n",
      "Epoch [6/10], Step [13/124], Loss: 0.0468\n",
      "Epoch [6/10], Step [14/124], Loss: 0.0260\n",
      "Epoch [6/10], Step [15/124], Loss: 0.0369\n",
      "Epoch [6/10], Step [16/124], Loss: 0.0222\n",
      "Epoch [6/10], Step [17/124], Loss: 0.0295\n",
      "Epoch [6/10], Step [18/124], Loss: 0.0335\n",
      "Epoch [6/10], Step [19/124], Loss: 0.0365\n",
      "Epoch [6/10], Step [20/124], Loss: 0.0234\n",
      "Epoch [6/10], Step [21/124], Loss: 0.0219\n",
      "Epoch [6/10], Step [22/124], Loss: 0.0187\n",
      "Epoch [6/10], Step [23/124], Loss: 0.0337\n",
      "Epoch [6/10], Step [24/124], Loss: 0.0366\n",
      "Epoch [6/10], Step [25/124], Loss: 0.0228\n",
      "Epoch [6/10], Step [26/124], Loss: 0.0238\n",
      "Epoch [6/10], Step [27/124], Loss: 0.0306\n",
      "Epoch [6/10], Step [28/124], Loss: 0.0402\n",
      "Epoch [6/10], Step [29/124], Loss: 0.0085\n",
      "Epoch [6/10], Step [30/124], Loss: 0.0252\n",
      "Epoch [6/10], Step [31/124], Loss: 0.0191\n",
      "Epoch [6/10], Step [32/124], Loss: 0.0690\n",
      "Epoch [6/10], Step [33/124], Loss: 0.0326\n",
      "Epoch [6/10], Step [34/124], Loss: 0.0341\n",
      "Epoch [6/10], Step [35/124], Loss: 0.0256\n",
      "Epoch [6/10], Step [36/124], Loss: 0.0406\n",
      "Epoch [6/10], Step [37/124], Loss: 0.0160\n",
      "Epoch [6/10], Step [38/124], Loss: 0.0128\n",
      "Epoch [6/10], Step [39/124], Loss: 0.0146\n",
      "Epoch [6/10], Step [40/124], Loss: 0.0093\n",
      "Epoch [6/10], Step [41/124], Loss: 0.0078\n",
      "Epoch [6/10], Step [42/124], Loss: 0.0146\n",
      "Epoch [6/10], Step [43/124], Loss: 0.0179\n",
      "Epoch [6/10], Step [44/124], Loss: 0.0047\n",
      "Epoch [6/10], Step [45/124], Loss: 0.0208\n",
      "Epoch [6/10], Step [46/124], Loss: 0.0220\n",
      "Epoch [6/10], Step [47/124], Loss: 0.0167\n",
      "Epoch [6/10], Step [48/124], Loss: 0.0236\n",
      "Epoch [6/10], Step [49/124], Loss: 0.0309\n",
      "Epoch [6/10], Step [50/124], Loss: 0.1086\n",
      "Epoch [6/10], Step [51/124], Loss: 0.0315\n",
      "Epoch [6/10], Step [52/124], Loss: 0.0436\n",
      "Epoch [6/10], Step [53/124], Loss: 0.0120\n",
      "Epoch [6/10], Step [54/124], Loss: 0.0427\n",
      "Epoch [6/10], Step [55/124], Loss: 0.0176\n",
      "Epoch [6/10], Step [56/124], Loss: 0.0133\n",
      "Epoch [6/10], Step [57/124], Loss: 0.0208\n",
      "Epoch [6/10], Step [58/124], Loss: 0.0400\n",
      "Epoch [6/10], Step [59/124], Loss: 0.0203\n",
      "Epoch [6/10], Step [60/124], Loss: 0.0537\n",
      "Epoch [6/10], Step [61/124], Loss: 0.0063\n",
      "Epoch [6/10], Step [62/124], Loss: 0.0150\n",
      "Epoch [6/10], Step [63/124], Loss: 0.0473\n",
      "Epoch [6/10], Step [64/124], Loss: 0.0214\n",
      "Epoch [6/10], Step [65/124], Loss: 0.0341\n",
      "Epoch [6/10], Step [66/124], Loss: 0.0247\n",
      "Epoch [6/10], Step [67/124], Loss: 0.0450\n",
      "Epoch [6/10], Step [68/124], Loss: 0.0392\n",
      "Epoch [6/10], Step [69/124], Loss: 0.0205\n",
      "Epoch [6/10], Step [70/124], Loss: 0.0253\n",
      "Epoch [6/10], Step [71/124], Loss: 0.0632\n",
      "Epoch [6/10], Step [72/124], Loss: 0.0117\n",
      "Epoch [6/10], Step [73/124], Loss: 0.0170\n",
      "Epoch [6/10], Step [74/124], Loss: 0.0274\n",
      "Epoch [6/10], Step [75/124], Loss: 0.0520\n",
      "Epoch [6/10], Step [76/124], Loss: 0.0400\n",
      "Epoch [6/10], Step [77/124], Loss: 0.0130\n",
      "Epoch [6/10], Step [78/124], Loss: 0.0162\n",
      "Epoch [6/10], Step [79/124], Loss: 0.0163\n",
      "Epoch [6/10], Step [80/124], Loss: 0.0293\n",
      "Epoch [6/10], Step [81/124], Loss: 0.0351\n",
      "Epoch [6/10], Step [82/124], Loss: 0.0233\n",
      "Epoch [6/10], Step [83/124], Loss: 0.0280\n",
      "Epoch [6/10], Step [84/124], Loss: 0.0353\n",
      "Epoch [6/10], Step [85/124], Loss: 0.0081\n",
      "Epoch [6/10], Step [86/124], Loss: 0.0232\n",
      "Epoch [6/10], Step [87/124], Loss: 0.0551\n",
      "Epoch [6/10], Step [88/124], Loss: 0.0362\n",
      "Epoch [6/10], Step [89/124], Loss: 0.0285\n",
      "Epoch [6/10], Step [90/124], Loss: 0.0584\n",
      "Epoch [6/10], Step [91/124], Loss: 0.0488\n",
      "Epoch [6/10], Step [92/124], Loss: 0.0268\n",
      "Epoch [6/10], Step [93/124], Loss: 0.0118\n",
      "Epoch [6/10], Step [94/124], Loss: 0.0556\n",
      "Epoch [6/10], Step [95/124], Loss: 0.0411\n",
      "Epoch [6/10], Step [96/124], Loss: 0.0412\n",
      "Epoch [6/10], Step [97/124], Loss: 0.0243\n",
      "Epoch [6/10], Step [98/124], Loss: 0.0423\n",
      "Epoch [6/10], Step [99/124], Loss: 0.0157\n",
      "Epoch [6/10], Step [100/124], Loss: 0.0192\n",
      "Epoch [6/10], Step [101/124], Loss: 0.0443\n",
      "Epoch [6/10], Step [102/124], Loss: 0.0308\n",
      "Epoch [6/10], Step [103/124], Loss: 0.0351\n",
      "Epoch [6/10], Step [104/124], Loss: 0.0743\n",
      "Epoch [6/10], Step [105/124], Loss: 0.0188\n",
      "Epoch [6/10], Step [106/124], Loss: 0.0308\n",
      "Epoch [6/10], Step [107/124], Loss: 0.0555\n",
      "Epoch [6/10], Step [108/124], Loss: 0.0185\n",
      "Epoch [6/10], Step [109/124], Loss: 0.0292\n",
      "Epoch [6/10], Step [110/124], Loss: 0.0125\n",
      "Epoch [6/10], Step [111/124], Loss: 0.0130\n",
      "Epoch [6/10], Step [112/124], Loss: 0.0263\n",
      "Epoch [6/10], Step [113/124], Loss: 0.0079\n",
      "Epoch [6/10], Step [114/124], Loss: 0.0197\n",
      "Epoch [6/10], Step [115/124], Loss: 0.0154\n",
      "Epoch [6/10], Step [116/124], Loss: 0.0186\n",
      "Epoch [6/10], Step [117/124], Loss: 0.0191\n",
      "Epoch [6/10], Step [118/124], Loss: 0.0421\n",
      "Epoch [6/10], Step [119/124], Loss: 0.0590\n",
      "Epoch [6/10], Step [120/124], Loss: 0.0461\n",
      "Epoch [6/10], Step [121/124], Loss: 0.0662\n",
      "Epoch [6/10], Step [122/124], Loss: 0.0221\n",
      "Epoch [6/10], Step [123/124], Loss: 0.0590\n",
      "Epoch [6/10], Step [124/124], Loss: 0.0040\n",
      "Epoch [7/10], Step [1/124], Loss: 0.0281\n",
      "Epoch [7/10], Step [2/124], Loss: 0.0183\n",
      "Epoch [7/10], Step [3/124], Loss: 0.0254\n",
      "Epoch [7/10], Step [4/124], Loss: 0.0240\n",
      "Epoch [7/10], Step [5/124], Loss: 0.0387\n",
      "Epoch [7/10], Step [6/124], Loss: 0.0110\n",
      "Epoch [7/10], Step [7/124], Loss: 0.0198\n",
      "Epoch [7/10], Step [8/124], Loss: 0.0229\n",
      "Epoch [7/10], Step [9/124], Loss: 0.0269\n",
      "Epoch [7/10], Step [10/124], Loss: 0.0134\n",
      "Epoch [7/10], Step [11/124], Loss: 0.0734\n",
      "Epoch [7/10], Step [12/124], Loss: 0.0429\n",
      "Epoch [7/10], Step [13/124], Loss: 0.0272\n",
      "Epoch [7/10], Step [14/124], Loss: 0.0223\n",
      "Epoch [7/10], Step [15/124], Loss: 0.0233\n",
      "Epoch [7/10], Step [16/124], Loss: 0.0169\n",
      "Epoch [7/10], Step [17/124], Loss: 0.0437\n",
      "Epoch [7/10], Step [18/124], Loss: 0.0571\n",
      "Epoch [7/10], Step [19/124], Loss: 0.0141\n",
      "Epoch [7/10], Step [20/124], Loss: 0.0311\n",
      "Epoch [7/10], Step [21/124], Loss: 0.0055\n",
      "Epoch [7/10], Step [22/124], Loss: 0.0113\n",
      "Epoch [7/10], Step [23/124], Loss: 0.0408\n",
      "Epoch [7/10], Step [24/124], Loss: 0.0135\n",
      "Epoch [7/10], Step [25/124], Loss: 0.0275\n",
      "Epoch [7/10], Step [26/124], Loss: 0.0238\n",
      "Epoch [7/10], Step [27/124], Loss: 0.0332\n",
      "Epoch [7/10], Step [28/124], Loss: 0.0508\n",
      "Epoch [7/10], Step [29/124], Loss: 0.0183\n",
      "Epoch [7/10], Step [30/124], Loss: 0.0528\n",
      "Epoch [7/10], Step [31/124], Loss: 0.0233\n",
      "Epoch [7/10], Step [32/124], Loss: 0.0120\n",
      "Epoch [7/10], Step [33/124], Loss: 0.0263\n",
      "Epoch [7/10], Step [34/124], Loss: 0.0373\n",
      "Epoch [7/10], Step [35/124], Loss: 0.0542\n",
      "Epoch [7/10], Step [36/124], Loss: 0.0117\n",
      "Epoch [7/10], Step [37/124], Loss: 0.0473\n",
      "Epoch [7/10], Step [38/124], Loss: 0.0375\n",
      "Epoch [7/10], Step [39/124], Loss: 0.0205\n",
      "Epoch [7/10], Step [40/124], Loss: 0.0198\n",
      "Epoch [7/10], Step [41/124], Loss: 0.0274\n",
      "Epoch [7/10], Step [42/124], Loss: 0.0261\n",
      "Epoch [7/10], Step [43/124], Loss: 0.0179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [44/124], Loss: 0.0309\n",
      "Epoch [7/10], Step [45/124], Loss: 0.0319\n",
      "Epoch [7/10], Step [46/124], Loss: 0.0114\n",
      "Epoch [7/10], Step [47/124], Loss: 0.0385\n",
      "Epoch [7/10], Step [48/124], Loss: 0.0404\n",
      "Epoch [7/10], Step [49/124], Loss: 0.0306\n",
      "Epoch [7/10], Step [50/124], Loss: 0.0127\n",
      "Epoch [7/10], Step [51/124], Loss: 0.0235\n",
      "Epoch [7/10], Step [52/124], Loss: 0.0955\n",
      "Epoch [7/10], Step [53/124], Loss: 0.0351\n",
      "Epoch [7/10], Step [54/124], Loss: 0.0091\n",
      "Epoch [7/10], Step [55/124], Loss: 0.0215\n",
      "Epoch [7/10], Step [56/124], Loss: 0.0290\n",
      "Epoch [7/10], Step [57/124], Loss: 0.0246\n",
      "Epoch [7/10], Step [58/124], Loss: 0.0137\n",
      "Epoch [7/10], Step [59/124], Loss: 0.0081\n",
      "Epoch [7/10], Step [60/124], Loss: 0.0372\n",
      "Epoch [7/10], Step [61/124], Loss: 0.0410\n",
      "Epoch [7/10], Step [62/124], Loss: 0.0133\n",
      "Epoch [7/10], Step [63/124], Loss: 0.0199\n",
      "Epoch [7/10], Step [64/124], Loss: 0.0288\n",
      "Epoch [7/10], Step [65/124], Loss: 0.0449\n",
      "Epoch [7/10], Step [66/124], Loss: 0.0386\n",
      "Epoch [7/10], Step [67/124], Loss: 0.0113\n",
      "Epoch [7/10], Step [68/124], Loss: 0.0189\n",
      "Epoch [7/10], Step [69/124], Loss: 0.0301\n",
      "Epoch [7/10], Step [70/124], Loss: 0.0071\n",
      "Epoch [7/10], Step [71/124], Loss: 0.0238\n",
      "Epoch [7/10], Step [72/124], Loss: 0.0314\n",
      "Epoch [7/10], Step [73/124], Loss: 0.0249\n",
      "Epoch [7/10], Step [74/124], Loss: 0.0269\n",
      "Epoch [7/10], Step [75/124], Loss: 0.0199\n",
      "Epoch [7/10], Step [76/124], Loss: 0.0078\n",
      "Epoch [7/10], Step [77/124], Loss: 0.0522\n",
      "Epoch [7/10], Step [78/124], Loss: 0.0151\n",
      "Epoch [7/10], Step [79/124], Loss: 0.0226\n",
      "Epoch [7/10], Step [80/124], Loss: 0.0168\n",
      "Epoch [7/10], Step [81/124], Loss: 0.0475\n",
      "Epoch [7/10], Step [82/124], Loss: 0.0318\n",
      "Epoch [7/10], Step [83/124], Loss: 0.0461\n",
      "Epoch [7/10], Step [84/124], Loss: 0.0399\n",
      "Epoch [7/10], Step [85/124], Loss: 0.0164\n",
      "Epoch [7/10], Step [86/124], Loss: 0.0134\n",
      "Epoch [7/10], Step [87/124], Loss: 0.0179\n",
      "Epoch [7/10], Step [88/124], Loss: 0.0182\n",
      "Epoch [7/10], Step [89/124], Loss: 0.0134\n",
      "Epoch [7/10], Step [90/124], Loss: 0.0345\n",
      "Epoch [7/10], Step [91/124], Loss: 0.0206\n",
      "Epoch [7/10], Step [92/124], Loss: 0.0110\n",
      "Epoch [7/10], Step [93/124], Loss: 0.0105\n",
      "Epoch [7/10], Step [94/124], Loss: 0.0435\n",
      "Epoch [7/10], Step [95/124], Loss: 0.0409\n",
      "Epoch [7/10], Step [96/124], Loss: 0.0316\n",
      "Epoch [7/10], Step [97/124], Loss: 0.0204\n",
      "Epoch [7/10], Step [98/124], Loss: 0.0118\n",
      "Epoch [7/10], Step [99/124], Loss: 0.0085\n",
      "Epoch [7/10], Step [100/124], Loss: 0.0158\n",
      "Epoch [7/10], Step [101/124], Loss: 0.0226\n",
      "Epoch [7/10], Step [102/124], Loss: 0.0205\n",
      "Epoch [7/10], Step [103/124], Loss: 0.0097\n",
      "Epoch [7/10], Step [104/124], Loss: 0.0180\n",
      "Epoch [7/10], Step [105/124], Loss: 0.0377\n",
      "Epoch [7/10], Step [106/124], Loss: 0.0180\n",
      "Epoch [7/10], Step [107/124], Loss: 0.0484\n",
      "Epoch [7/10], Step [108/124], Loss: 0.0202\n",
      "Epoch [7/10], Step [109/124], Loss: 0.0127\n",
      "Epoch [7/10], Step [110/124], Loss: 0.0241\n",
      "Epoch [7/10], Step [111/124], Loss: 0.0517\n",
      "Epoch [7/10], Step [112/124], Loss: 0.0382\n",
      "Epoch [7/10], Step [113/124], Loss: 0.0792\n",
      "Epoch [7/10], Step [114/124], Loss: 0.0076\n",
      "Epoch [7/10], Step [115/124], Loss: 0.0189\n",
      "Epoch [7/10], Step [116/124], Loss: 0.0282\n",
      "Epoch [7/10], Step [117/124], Loss: 0.0314\n",
      "Epoch [7/10], Step [118/124], Loss: 0.0231\n",
      "Epoch [7/10], Step [119/124], Loss: 0.0208\n",
      "Epoch [7/10], Step [120/124], Loss: 0.0183\n",
      "Epoch [7/10], Step [121/124], Loss: 0.0390\n",
      "Epoch [7/10], Step [122/124], Loss: 0.0092\n",
      "Epoch [7/10], Step [123/124], Loss: 0.0417\n",
      "Epoch [7/10], Step [124/124], Loss: 0.0006\n",
      "Epoch [8/10], Step [1/124], Loss: 0.0290\n",
      "Epoch [8/10], Step [2/124], Loss: 0.0184\n",
      "Epoch [8/10], Step [3/124], Loss: 0.0145\n",
      "Epoch [8/10], Step [4/124], Loss: 0.0462\n",
      "Epoch [8/10], Step [5/124], Loss: 0.0476\n",
      "Epoch [8/10], Step [6/124], Loss: 0.0225\n",
      "Epoch [8/10], Step [7/124], Loss: 0.0146\n",
      "Epoch [8/10], Step [8/124], Loss: 0.0376\n",
      "Epoch [8/10], Step [9/124], Loss: 0.0300\n",
      "Epoch [8/10], Step [10/124], Loss: 0.0245\n",
      "Epoch [8/10], Step [11/124], Loss: 0.0177\n",
      "Epoch [8/10], Step [12/124], Loss: 0.0382\n",
      "Epoch [8/10], Step [13/124], Loss: 0.0249\n",
      "Epoch [8/10], Step [14/124], Loss: 0.0197\n",
      "Epoch [8/10], Step [15/124], Loss: 0.0143\n",
      "Epoch [8/10], Step [16/124], Loss: 0.0263\n",
      "Epoch [8/10], Step [17/124], Loss: 0.0150\n",
      "Epoch [8/10], Step [18/124], Loss: 0.0049\n",
      "Epoch [8/10], Step [19/124], Loss: 0.0148\n",
      "Epoch [8/10], Step [20/124], Loss: 0.0141\n",
      "Epoch [8/10], Step [21/124], Loss: 0.0198\n",
      "Epoch [8/10], Step [22/124], Loss: 0.0164\n",
      "Epoch [8/10], Step [23/124], Loss: 0.0162\n",
      "Epoch [8/10], Step [24/124], Loss: 0.0221\n",
      "Epoch [8/10], Step [25/124], Loss: 0.0242\n",
      "Epoch [8/10], Step [26/124], Loss: 0.0228\n",
      "Epoch [8/10], Step [27/124], Loss: 0.0115\n",
      "Epoch [8/10], Step [28/124], Loss: 0.0097\n",
      "Epoch [8/10], Step [29/124], Loss: 0.0245\n",
      "Epoch [8/10], Step [30/124], Loss: 0.0258\n",
      "Epoch [8/10], Step [31/124], Loss: 0.0257\n",
      "Epoch [8/10], Step [32/124], Loss: 0.0218\n",
      "Epoch [8/10], Step [33/124], Loss: 0.0156\n",
      "Epoch [8/10], Step [34/124], Loss: 0.0092\n",
      "Epoch [8/10], Step [35/124], Loss: 0.0238\n",
      "Epoch [8/10], Step [36/124], Loss: 0.0133\n",
      "Epoch [8/10], Step [37/124], Loss: 0.0168\n",
      "Epoch [8/10], Step [38/124], Loss: 0.0170\n",
      "Epoch [8/10], Step [39/124], Loss: 0.0118\n",
      "Epoch [8/10], Step [40/124], Loss: 0.0166\n",
      "Epoch [8/10], Step [41/124], Loss: 0.0878\n",
      "Epoch [8/10], Step [42/124], Loss: 0.0177\n",
      "Epoch [8/10], Step [43/124], Loss: 0.0062\n",
      "Epoch [8/10], Step [44/124], Loss: 0.0214\n",
      "Epoch [8/10], Step [45/124], Loss: 0.0105\n",
      "Epoch [8/10], Step [46/124], Loss: 0.0298\n",
      "Epoch [8/10], Step [47/124], Loss: 0.0255\n",
      "Epoch [8/10], Step [48/124], Loss: 0.0260\n",
      "Epoch [8/10], Step [49/124], Loss: 0.0564\n",
      "Epoch [8/10], Step [50/124], Loss: 0.0346\n",
      "Epoch [8/10], Step [51/124], Loss: 0.0073\n",
      "Epoch [8/10], Step [52/124], Loss: 0.0136\n",
      "Epoch [8/10], Step [53/124], Loss: 0.0090\n",
      "Epoch [8/10], Step [54/124], Loss: 0.0074\n",
      "Epoch [8/10], Step [55/124], Loss: 0.0298\n",
      "Epoch [8/10], Step [56/124], Loss: 0.0506\n",
      "Epoch [8/10], Step [57/124], Loss: 0.0374\n",
      "Epoch [8/10], Step [58/124], Loss: 0.0188\n",
      "Epoch [8/10], Step [59/124], Loss: 0.0299\n",
      "Epoch [8/10], Step [60/124], Loss: 0.0164\n",
      "Epoch [8/10], Step [61/124], Loss: 0.0279\n",
      "Epoch [8/10], Step [62/124], Loss: 0.0164\n",
      "Epoch [8/10], Step [63/124], Loss: 0.0283\n",
      "Epoch [8/10], Step [64/124], Loss: 0.0249\n",
      "Epoch [8/10], Step [65/124], Loss: 0.0102\n",
      "Epoch [8/10], Step [66/124], Loss: 0.0250\n",
      "Epoch [8/10], Step [67/124], Loss: 0.0051\n",
      "Epoch [8/10], Step [68/124], Loss: 0.0051\n",
      "Epoch [8/10], Step [69/124], Loss: 0.0204\n",
      "Epoch [8/10], Step [70/124], Loss: 0.0252\n",
      "Epoch [8/10], Step [71/124], Loss: 0.0269\n",
      "Epoch [8/10], Step [72/124], Loss: 0.0117\n",
      "Epoch [8/10], Step [73/124], Loss: 0.0122\n",
      "Epoch [8/10], Step [74/124], Loss: 0.0151\n",
      "Epoch [8/10], Step [75/124], Loss: 0.0427\n",
      "Epoch [8/10], Step [76/124], Loss: 0.0522\n",
      "Epoch [8/10], Step [77/124], Loss: 0.0156\n",
      "Epoch [8/10], Step [78/124], Loss: 0.0106\n",
      "Epoch [8/10], Step [79/124], Loss: 0.0333\n",
      "Epoch [8/10], Step [80/124], Loss: 0.0321\n",
      "Epoch [8/10], Step [81/124], Loss: 0.0272\n",
      "Epoch [8/10], Step [82/124], Loss: 0.0400\n",
      "Epoch [8/10], Step [83/124], Loss: 0.0518\n",
      "Epoch [8/10], Step [84/124], Loss: 0.0365\n",
      "Epoch [8/10], Step [85/124], Loss: 0.0231\n",
      "Epoch [8/10], Step [86/124], Loss: 0.0196\n",
      "Epoch [8/10], Step [87/124], Loss: 0.0251\n",
      "Epoch [8/10], Step [88/124], Loss: 0.0158\n",
      "Epoch [8/10], Step [89/124], Loss: 0.0125\n",
      "Epoch [8/10], Step [90/124], Loss: 0.0258\n",
      "Epoch [8/10], Step [91/124], Loss: 0.0513\n",
      "Epoch [8/10], Step [92/124], Loss: 0.0435\n",
      "Epoch [8/10], Step [93/124], Loss: 0.0570\n",
      "Epoch [8/10], Step [94/124], Loss: 0.0239\n",
      "Epoch [8/10], Step [95/124], Loss: 0.0269\n",
      "Epoch [8/10], Step [96/124], Loss: 0.0339\n",
      "Epoch [8/10], Step [97/124], Loss: 0.0179\n",
      "Epoch [8/10], Step [98/124], Loss: 0.0167\n",
      "Epoch [8/10], Step [99/124], Loss: 0.0220\n",
      "Epoch [8/10], Step [100/124], Loss: 0.0364\n",
      "Epoch [8/10], Step [101/124], Loss: 0.0215\n",
      "Epoch [8/10], Step [102/124], Loss: 0.0113\n",
      "Epoch [8/10], Step [103/124], Loss: 0.0251\n",
      "Epoch [8/10], Step [104/124], Loss: 0.0203\n",
      "Epoch [8/10], Step [105/124], Loss: 0.0090\n",
      "Epoch [8/10], Step [106/124], Loss: 0.0148\n",
      "Epoch [8/10], Step [107/124], Loss: 0.0086\n",
      "Epoch [8/10], Step [108/124], Loss: 0.0287\n",
      "Epoch [8/10], Step [109/124], Loss: 0.0144\n",
      "Epoch [8/10], Step [110/124], Loss: 0.0322\n",
      "Epoch [8/10], Step [111/124], Loss: 0.0278\n",
      "Epoch [8/10], Step [112/124], Loss: 0.0341\n",
      "Epoch [8/10], Step [113/124], Loss: 0.0092\n",
      "Epoch [8/10], Step [114/124], Loss: 0.0188\n",
      "Epoch [8/10], Step [115/124], Loss: 0.0225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [116/124], Loss: 0.0530\n",
      "Epoch [8/10], Step [117/124], Loss: 0.0189\n",
      "Epoch [8/10], Step [118/124], Loss: 0.0482\n",
      "Epoch [8/10], Step [119/124], Loss: 0.0128\n",
      "Epoch [8/10], Step [120/124], Loss: 0.0371\n",
      "Epoch [8/10], Step [121/124], Loss: 0.0495\n",
      "Epoch [8/10], Step [122/124], Loss: 0.0342\n",
      "Epoch [8/10], Step [123/124], Loss: 0.0163\n",
      "Epoch [8/10], Step [124/124], Loss: 0.0002\n",
      "Epoch [9/10], Step [1/124], Loss: 0.0167\n",
      "Epoch [9/10], Step [2/124], Loss: 0.0151\n",
      "Epoch [9/10], Step [3/124], Loss: 0.0389\n",
      "Epoch [9/10], Step [4/124], Loss: 0.0115\n",
      "Epoch [9/10], Step [5/124], Loss: 0.0463\n",
      "Epoch [9/10], Step [6/124], Loss: 0.0135\n",
      "Epoch [9/10], Step [7/124], Loss: 0.0162\n",
      "Epoch [9/10], Step [8/124], Loss: 0.0325\n",
      "Epoch [9/10], Step [9/124], Loss: 0.0159\n",
      "Epoch [9/10], Step [10/124], Loss: 0.0167\n",
      "Epoch [9/10], Step [11/124], Loss: 0.0478\n",
      "Epoch [9/10], Step [12/124], Loss: 0.0186\n",
      "Epoch [9/10], Step [13/124], Loss: 0.0249\n",
      "Epoch [9/10], Step [14/124], Loss: 0.0264\n",
      "Epoch [9/10], Step [15/124], Loss: 0.0106\n",
      "Epoch [9/10], Step [16/124], Loss: 0.0280\n",
      "Epoch [9/10], Step [17/124], Loss: 0.0134\n",
      "Epoch [9/10], Step [18/124], Loss: 0.0060\n",
      "Epoch [9/10], Step [19/124], Loss: 0.0060\n",
      "Epoch [9/10], Step [20/124], Loss: 0.0487\n",
      "Epoch [9/10], Step [21/124], Loss: 0.0312\n",
      "Epoch [9/10], Step [22/124], Loss: 0.0145\n",
      "Epoch [9/10], Step [23/124], Loss: 0.0252\n",
      "Epoch [9/10], Step [24/124], Loss: 0.0146\n",
      "Epoch [9/10], Step [25/124], Loss: 0.0200\n",
      "Epoch [9/10], Step [26/124], Loss: 0.0089\n",
      "Epoch [9/10], Step [27/124], Loss: 0.0119\n",
      "Epoch [9/10], Step [28/124], Loss: 0.0153\n",
      "Epoch [9/10], Step [29/124], Loss: 0.0190\n",
      "Epoch [9/10], Step [30/124], Loss: 0.0257\n",
      "Epoch [9/10], Step [31/124], Loss: 0.0285\n",
      "Epoch [9/10], Step [32/124], Loss: 0.0095\n",
      "Epoch [9/10], Step [33/124], Loss: 0.0062\n",
      "Epoch [9/10], Step [34/124], Loss: 0.0098\n",
      "Epoch [9/10], Step [35/124], Loss: 0.0297\n",
      "Epoch [9/10], Step [36/124], Loss: 0.0105\n",
      "Epoch [9/10], Step [37/124], Loss: 0.0299\n",
      "Epoch [9/10], Step [38/124], Loss: 0.0157\n",
      "Epoch [9/10], Step [39/124], Loss: 0.0086\n",
      "Epoch [9/10], Step [40/124], Loss: 0.0316\n",
      "Epoch [9/10], Step [41/124], Loss: 0.0331\n",
      "Epoch [9/10], Step [42/124], Loss: 0.0047\n",
      "Epoch [9/10], Step [43/124], Loss: 0.0095\n",
      "Epoch [9/10], Step [44/124], Loss: 0.0558\n",
      "Epoch [9/10], Step [45/124], Loss: 0.0370\n",
      "Epoch [9/10], Step [46/124], Loss: 0.0082\n",
      "Epoch [9/10], Step [47/124], Loss: 0.0129\n",
      "Epoch [9/10], Step [48/124], Loss: 0.0185\n",
      "Epoch [9/10], Step [49/124], Loss: 0.0314\n",
      "Epoch [9/10], Step [50/124], Loss: 0.0155\n",
      "Epoch [9/10], Step [51/124], Loss: 0.0212\n",
      "Epoch [9/10], Step [52/124], Loss: 0.0103\n",
      "Epoch [9/10], Step [53/124], Loss: 0.0060\n",
      "Epoch [9/10], Step [54/124], Loss: 0.0119\n",
      "Epoch [9/10], Step [55/124], Loss: 0.0349\n",
      "Epoch [9/10], Step [56/124], Loss: 0.0400\n",
      "Epoch [9/10], Step [57/124], Loss: 0.0273\n",
      "Epoch [9/10], Step [58/124], Loss: 0.0192\n",
      "Epoch [9/10], Step [59/124], Loss: 0.0165\n",
      "Epoch [9/10], Step [60/124], Loss: 0.0121\n",
      "Epoch [9/10], Step [61/124], Loss: 0.0481\n",
      "Epoch [9/10], Step [62/124], Loss: 0.0137\n",
      "Epoch [9/10], Step [63/124], Loss: 0.0065\n",
      "Epoch [9/10], Step [64/124], Loss: 0.0341\n",
      "Epoch [9/10], Step [65/124], Loss: 0.0363\n",
      "Epoch [9/10], Step [66/124], Loss: 0.0339\n",
      "Epoch [9/10], Step [67/124], Loss: 0.0098\n",
      "Epoch [9/10], Step [68/124], Loss: 0.0194\n",
      "Epoch [9/10], Step [69/124], Loss: 0.0166\n",
      "Epoch [9/10], Step [70/124], Loss: 0.0141\n",
      "Epoch [9/10], Step [71/124], Loss: 0.0149\n",
      "Epoch [9/10], Step [72/124], Loss: 0.0303\n",
      "Epoch [9/10], Step [73/124], Loss: 0.0510\n",
      "Epoch [9/10], Step [74/124], Loss: 0.0360\n",
      "Epoch [9/10], Step [75/124], Loss: 0.0150\n",
      "Epoch [9/10], Step [76/124], Loss: 0.0079\n",
      "Epoch [9/10], Step [77/124], Loss: 0.0111\n",
      "Epoch [9/10], Step [78/124], Loss: 0.0083\n",
      "Epoch [9/10], Step [79/124], Loss: 0.0278\n",
      "Epoch [9/10], Step [80/124], Loss: 0.0154\n",
      "Epoch [9/10], Step [81/124], Loss: 0.0153\n",
      "Epoch [9/10], Step [82/124], Loss: 0.0231\n",
      "Epoch [9/10], Step [83/124], Loss: 0.0072\n",
      "Epoch [9/10], Step [84/124], Loss: 0.0213\n",
      "Epoch [9/10], Step [85/124], Loss: 0.0164\n",
      "Epoch [9/10], Step [86/124], Loss: 0.0132\n",
      "Epoch [9/10], Step [87/124], Loss: 0.0569\n",
      "Epoch [9/10], Step [88/124], Loss: 0.0219\n",
      "Epoch [9/10], Step [89/124], Loss: 0.0331\n",
      "Epoch [9/10], Step [90/124], Loss: 0.0066\n",
      "Epoch [9/10], Step [91/124], Loss: 0.0382\n",
      "Epoch [9/10], Step [92/124], Loss: 0.0047\n",
      "Epoch [9/10], Step [93/124], Loss: 0.0176\n",
      "Epoch [9/10], Step [94/124], Loss: 0.0138\n",
      "Epoch [9/10], Step [95/124], Loss: 0.0233\n",
      "Epoch [9/10], Step [96/124], Loss: 0.0380\n",
      "Epoch [9/10], Step [97/124], Loss: 0.0392\n",
      "Epoch [9/10], Step [98/124], Loss: 0.0306\n",
      "Epoch [9/10], Step [99/124], Loss: 0.0253\n",
      "Epoch [9/10], Step [100/124], Loss: 0.0359\n",
      "Epoch [9/10], Step [101/124], Loss: 0.0422\n",
      "Epoch [9/10], Step [102/124], Loss: 0.0201\n",
      "Epoch [9/10], Step [103/124], Loss: 0.0922\n",
      "Epoch [9/10], Step [104/124], Loss: 0.0318\n",
      "Epoch [9/10], Step [105/124], Loss: 0.0312\n",
      "Epoch [9/10], Step [106/124], Loss: 0.0331\n",
      "Epoch [9/10], Step [107/124], Loss: 0.0204\n",
      "Epoch [9/10], Step [108/124], Loss: 0.0233\n",
      "Epoch [9/10], Step [109/124], Loss: 0.0271\n",
      "Epoch [9/10], Step [110/124], Loss: 0.0172\n",
      "Epoch [9/10], Step [111/124], Loss: 0.0106\n",
      "Epoch [9/10], Step [112/124], Loss: 0.0113\n",
      "Epoch [9/10], Step [113/124], Loss: 0.0416\n",
      "Epoch [9/10], Step [114/124], Loss: 0.0092\n",
      "Epoch [9/10], Step [115/124], Loss: 0.0300\n",
      "Epoch [9/10], Step [116/124], Loss: 0.0291\n",
      "Epoch [9/10], Step [117/124], Loss: 0.0193\n",
      "Epoch [9/10], Step [118/124], Loss: 0.0284\n",
      "Epoch [9/10], Step [119/124], Loss: 0.0174\n",
      "Epoch [9/10], Step [120/124], Loss: 0.0200\n",
      "Epoch [9/10], Step [121/124], Loss: 0.0181\n",
      "Epoch [9/10], Step [122/124], Loss: 0.0123\n",
      "Epoch [9/10], Step [123/124], Loss: 0.0263\n",
      "Epoch [9/10], Step [124/124], Loss: 0.0006\n",
      "Epoch [10/10], Step [1/124], Loss: 0.0225\n",
      "Epoch [10/10], Step [2/124], Loss: 0.0494\n",
      "Epoch [10/10], Step [3/124], Loss: 0.0396\n",
      "Epoch [10/10], Step [4/124], Loss: 0.0166\n",
      "Epoch [10/10], Step [5/124], Loss: 0.0213\n",
      "Epoch [10/10], Step [6/124], Loss: 0.0149\n",
      "Epoch [10/10], Step [7/124], Loss: 0.0258\n",
      "Epoch [10/10], Step [8/124], Loss: 0.0490\n",
      "Epoch [10/10], Step [9/124], Loss: 0.0320\n",
      "Epoch [10/10], Step [10/124], Loss: 0.0194\n",
      "Epoch [10/10], Step [11/124], Loss: 0.0158\n",
      "Epoch [10/10], Step [12/124], Loss: 0.0432\n",
      "Epoch [10/10], Step [13/124], Loss: 0.0260\n",
      "Epoch [10/10], Step [14/124], Loss: 0.0197\n",
      "Epoch [10/10], Step [15/124], Loss: 0.0046\n",
      "Epoch [10/10], Step [16/124], Loss: 0.0237\n",
      "Epoch [10/10], Step [17/124], Loss: 0.0599\n",
      "Epoch [10/10], Step [18/124], Loss: 0.0211\n",
      "Epoch [10/10], Step [19/124], Loss: 0.0139\n",
      "Epoch [10/10], Step [20/124], Loss: 0.0272\n",
      "Epoch [10/10], Step [21/124], Loss: 0.0280\n",
      "Epoch [10/10], Step [22/124], Loss: 0.0289\n",
      "Epoch [10/10], Step [23/124], Loss: 0.0238\n",
      "Epoch [10/10], Step [24/124], Loss: 0.0326\n",
      "Epoch [10/10], Step [25/124], Loss: 0.0080\n",
      "Epoch [10/10], Step [26/124], Loss: 0.0366\n",
      "Epoch [10/10], Step [27/124], Loss: 0.0208\n",
      "Epoch [10/10], Step [28/124], Loss: 0.0108\n",
      "Epoch [10/10], Step [29/124], Loss: 0.0107\n",
      "Epoch [10/10], Step [30/124], Loss: 0.0080\n",
      "Epoch [10/10], Step [31/124], Loss: 0.0106\n",
      "Epoch [10/10], Step [32/124], Loss: 0.0181\n",
      "Epoch [10/10], Step [33/124], Loss: 0.0075\n",
      "Epoch [10/10], Step [34/124], Loss: 0.0127\n",
      "Epoch [10/10], Step [35/124], Loss: 0.0215\n",
      "Epoch [10/10], Step [36/124], Loss: 0.0058\n",
      "Epoch [10/10], Step [37/124], Loss: 0.0205\n",
      "Epoch [10/10], Step [38/124], Loss: 0.0271\n",
      "Epoch [10/10], Step [39/124], Loss: 0.0132\n",
      "Epoch [10/10], Step [40/124], Loss: 0.0232\n",
      "Epoch [10/10], Step [41/124], Loss: 0.0065\n",
      "Epoch [10/10], Step [42/124], Loss: 0.0092\n",
      "Epoch [10/10], Step [43/124], Loss: 0.0109\n",
      "Epoch [10/10], Step [44/124], Loss: 0.0076\n",
      "Epoch [10/10], Step [45/124], Loss: 0.0285\n",
      "Epoch [10/10], Step [46/124], Loss: 0.0172\n",
      "Epoch [10/10], Step [47/124], Loss: 0.0122\n",
      "Epoch [10/10], Step [48/124], Loss: 0.0126\n",
      "Epoch [10/10], Step [49/124], Loss: 0.0416\n",
      "Epoch [10/10], Step [50/124], Loss: 0.0172\n",
      "Epoch [10/10], Step [51/124], Loss: 0.0176\n",
      "Epoch [10/10], Step [52/124], Loss: 0.0162\n",
      "Epoch [10/10], Step [53/124], Loss: 0.0356\n",
      "Epoch [10/10], Step [54/124], Loss: 0.0308\n",
      "Epoch [10/10], Step [55/124], Loss: 0.0115\n",
      "Epoch [10/10], Step [56/124], Loss: 0.0336\n",
      "Epoch [10/10], Step [57/124], Loss: 0.0116\n",
      "Epoch [10/10], Step [58/124], Loss: 0.0165\n",
      "Epoch [10/10], Step [59/124], Loss: 0.0150\n",
      "Epoch [10/10], Step [60/124], Loss: 0.0133\n",
      "Epoch [10/10], Step [61/124], Loss: 0.0059\n",
      "Epoch [10/10], Step [62/124], Loss: 0.0141\n",
      "Epoch [10/10], Step [63/124], Loss: 0.0143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [64/124], Loss: 0.0230\n",
      "Epoch [10/10], Step [65/124], Loss: 0.0145\n",
      "Epoch [10/10], Step [66/124], Loss: 0.0171\n",
      "Epoch [10/10], Step [67/124], Loss: 0.0225\n",
      "Epoch [10/10], Step [68/124], Loss: 0.0108\n",
      "Epoch [10/10], Step [69/124], Loss: 0.0215\n",
      "Epoch [10/10], Step [70/124], Loss: 0.0087\n",
      "Epoch [10/10], Step [71/124], Loss: 0.0513\n",
      "Epoch [10/10], Step [72/124], Loss: 0.0222\n",
      "Epoch [10/10], Step [73/124], Loss: 0.0059\n",
      "Epoch [10/10], Step [74/124], Loss: 0.0100\n",
      "Epoch [10/10], Step [75/124], Loss: 0.0176\n",
      "Epoch [10/10], Step [76/124], Loss: 0.0345\n",
      "Epoch [10/10], Step [77/124], Loss: 0.0282\n",
      "Epoch [10/10], Step [78/124], Loss: 0.0079\n",
      "Epoch [10/10], Step [79/124], Loss: 0.0119\n",
      "Epoch [10/10], Step [80/124], Loss: 0.0109\n",
      "Epoch [10/10], Step [81/124], Loss: 0.0268\n",
      "Epoch [10/10], Step [82/124], Loss: 0.0105\n",
      "Epoch [10/10], Step [83/124], Loss: 0.0256\n",
      "Epoch [10/10], Step [84/124], Loss: 0.0076\n",
      "Epoch [10/10], Step [85/124], Loss: 0.0069\n",
      "Epoch [10/10], Step [86/124], Loss: 0.0081\n",
      "Epoch [10/10], Step [87/124], Loss: 0.0099\n",
      "Epoch [10/10], Step [88/124], Loss: 0.0221\n",
      "Epoch [10/10], Step [89/124], Loss: 0.0123\n",
      "Epoch [10/10], Step [90/124], Loss: 0.0067\n",
      "Epoch [10/10], Step [91/124], Loss: 0.0057\n",
      "Epoch [10/10], Step [92/124], Loss: 0.0211\n",
      "Epoch [10/10], Step [93/124], Loss: 0.0086\n",
      "Epoch [10/10], Step [94/124], Loss: 0.0091\n",
      "Epoch [10/10], Step [95/124], Loss: 0.0272\n",
      "Epoch [10/10], Step [96/124], Loss: 0.0141\n",
      "Epoch [10/10], Step [97/124], Loss: 0.0164\n",
      "Epoch [10/10], Step [98/124], Loss: 0.0193\n",
      "Epoch [10/10], Step [99/124], Loss: 0.0182\n",
      "Epoch [10/10], Step [100/124], Loss: 0.0233\n",
      "Epoch [10/10], Step [101/124], Loss: 0.0084\n",
      "Epoch [10/10], Step [102/124], Loss: 0.0438\n",
      "Epoch [10/10], Step [103/124], Loss: 0.0197\n",
      "Epoch [10/10], Step [104/124], Loss: 0.0140\n",
      "Epoch [10/10], Step [105/124], Loss: 0.0221\n",
      "Epoch [10/10], Step [106/124], Loss: 0.0314\n",
      "Epoch [10/10], Step [107/124], Loss: 0.0070\n",
      "Epoch [10/10], Step [108/124], Loss: 0.0175\n",
      "Epoch [10/10], Step [109/124], Loss: 0.0054\n",
      "Epoch [10/10], Step [110/124], Loss: 0.0490\n",
      "Epoch [10/10], Step [111/124], Loss: 0.0394\n",
      "Epoch [10/10], Step [112/124], Loss: 0.0155\n",
      "Epoch [10/10], Step [113/124], Loss: 0.0195\n",
      "Epoch [10/10], Step [114/124], Loss: 0.0217\n",
      "Epoch [10/10], Step [115/124], Loss: 0.0265\n",
      "Epoch [10/10], Step [116/124], Loss: 0.0324\n",
      "Epoch [10/10], Step [117/124], Loss: 0.0244\n",
      "Epoch [10/10], Step [118/124], Loss: 0.0123\n",
      "Epoch [10/10], Step [119/124], Loss: 0.0423\n",
      "Epoch [10/10], Step [120/124], Loss: 0.0320\n",
      "Epoch [10/10], Step [121/124], Loss: 0.0530\n",
      "Epoch [10/10], Step [122/124], Loss: 0.0393\n",
      "Epoch [10/10], Step [123/124], Loss: 0.0108\n",
      "Epoch [10/10], Step [124/124], Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range (epochs):\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "       \n",
    "        #forward\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        \n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #Backpropagate the prediction loss with a call to loss.backwards()\n",
    "        \n",
    "        optimizer.step() # to adjust the gradients received from backward pass\n",
    "       \n",
    "        print (f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:28.933438Z",
     "iopub.status.busy": "2021-12-10T03:50:28.933201Z",
     "iopub.status.idle": "2021-12-10T03:50:28.940632Z",
     "shell.execute_reply": "2021-12-10T03:50:28.939536Z",
     "shell.execute_reply.started": "2021-12-10T03:50:28.933409Z"
    },
    "id": "zRMsuxvqXPc7"
   },
   "outputs": [],
   "source": [
    "def check_acc (loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            outputs = model(x)\n",
    "            _, pred = outputs.max(1)\n",
    "            num_correct += (pred==y).sum()\n",
    "            num_samples += pred.size(0)\n",
    "            print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct/float(num_samples)*100):.2f}')\n",
    "        \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:50:28.942125Z",
     "iopub.status.busy": "2021-12-10T03:50:28.941884Z",
     "iopub.status.idle": "2021-12-10T03:50:30.655117Z",
     "shell.execute_reply": "2021-12-10T03:50:30.654161Z",
     "shell.execute_reply.started": "2021-12-10T03:50:28.942095Z"
    },
    "id": "ai_2_LTIcpza",
    "outputId": "217fd1e6-ef30-4ad7-93d0-07b5d79a25a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 253/256 with accuracy 98.83\n",
      "Got 505/512 with accuracy 98.63\n",
      "Got 756/768 with accuracy 98.44\n",
      "Got 1009/1024 with accuracy 98.54\n",
      "Got 1262/1280 with accuracy 98.59\n",
      "Got 1514/1536 with accuracy 98.57\n",
      "Got 1764/1792 with accuracy 98.44\n",
      "Got 2014/2048 with accuracy 98.34\n",
      "Got 2268/2304 with accuracy 98.44\n",
      "Got 2520/2560 with accuracy 98.44\n",
      "Got 2770/2816 with accuracy 98.37\n",
      "Got 3019/3072 with accuracy 98.27\n",
      "Got 3272/3328 with accuracy 98.32\n",
      "Got 3525/3584 with accuracy 98.35\n",
      "Got 3778/3840 with accuracy 98.39\n",
      "Got 4031/4096 with accuracy 98.41\n",
      "Got 4284/4352 with accuracy 98.44\n",
      "Got 4536/4608 with accuracy 98.44\n",
      "Got 4786/4864 with accuracy 98.40\n",
      "Got 5040/5120 with accuracy 98.44\n",
      "Got 5291/5376 with accuracy 98.42\n",
      "Got 5545/5632 with accuracy 98.46\n",
      "Got 5797/5888 with accuracy 98.45\n",
      "Got 6050/6144 with accuracy 98.47\n",
      "Got 6306/6400 with accuracy 98.53\n",
      "Got 6559/6656 with accuracy 98.54\n",
      "Got 6810/6912 with accuracy 98.52\n",
      "Got 7062/7168 with accuracy 98.52\n",
      "Got 7311/7424 with accuracy 98.48\n",
      "Got 7562/7680 with accuracy 98.46\n",
      "Got 7811/7936 with accuracy 98.42\n",
      "Got 8062/8192 with accuracy 98.41\n",
      "Got 8316/8448 with accuracy 98.44\n",
      "Got 8565/8704 with accuracy 98.40\n",
      "Got 8816/8960 with accuracy 98.39\n",
      "Got 9067/9216 with accuracy 98.38\n",
      "Got 9321/9472 with accuracy 98.41\n",
      "Got 9575/9728 with accuracy 98.43\n",
      "Got 9825/9984 with accuracy 98.41\n",
      "Got 10076/10240 with accuracy 98.40\n",
      "Got 10329/10496 with accuracy 98.41\n",
      "Got 10579/10752 with accuracy 98.39\n",
      "Got 10831/11008 with accuracy 98.39\n",
      "Got 11082/11264 with accuracy 98.38\n",
      "Got 11335/11520 with accuracy 98.39\n",
      "Got 11589/11776 with accuracy 98.41\n",
      "Got 11842/12032 with accuracy 98.42\n",
      "Got 12095/12288 with accuracy 98.43\n",
      "Got 12346/12544 with accuracy 98.42\n",
      "Got 12600/12800 with accuracy 98.44\n",
      "Got 12856/13056 with accuracy 98.47\n",
      "Got 13110/13312 with accuracy 98.48\n",
      "Got 13361/13568 with accuracy 98.47\n",
      "Got 13617/13824 with accuracy 98.50\n",
      "Got 13871/14080 with accuracy 98.52\n",
      "Got 14125/14336 with accuracy 98.53\n",
      "Got 14377/14592 with accuracy 98.53\n",
      "Got 14633/14848 with accuracy 98.55\n",
      "Got 14885/15104 with accuracy 98.55\n",
      "Got 15136/15360 with accuracy 98.54\n",
      "Got 15390/15616 with accuracy 98.55\n",
      "Got 15643/15872 with accuracy 98.56\n",
      "Got 15897/16128 with accuracy 98.57\n",
      "Got 16146/16384 with accuracy 98.55\n",
      "Got 16398/16640 with accuracy 98.55\n",
      "Got 16650/16896 with accuracy 98.54\n",
      "Got 16901/17152 with accuracy 98.54\n",
      "Got 17150/17408 with accuracy 98.52\n",
      "Got 17404/17664 with accuracy 98.53\n",
      "Got 17656/17920 with accuracy 98.53\n",
      "Got 17907/18176 with accuracy 98.52\n",
      "Got 18156/18432 with accuracy 98.50\n",
      "Got 18404/18688 with accuracy 98.48\n",
      "Got 18654/18944 with accuracy 98.47\n",
      "Got 18906/19200 with accuracy 98.47\n",
      "Got 19155/19456 with accuracy 98.45\n",
      "Got 19409/19712 with accuracy 98.46\n",
      "Got 19662/19968 with accuracy 98.47\n",
      "Got 19913/20224 with accuracy 98.46\n",
      "Got 20165/20480 with accuracy 98.46\n",
      "Got 20413/20736 with accuracy 98.44\n",
      "Got 20664/20992 with accuracy 98.44\n",
      "Got 20918/21248 with accuracy 98.45\n",
      "Got 21167/21504 with accuracy 98.43\n",
      "Got 21422/21760 with accuracy 98.45\n",
      "Got 21675/22016 with accuracy 98.45\n",
      "Got 21928/22272 with accuracy 98.46\n",
      "Got 22178/22528 with accuracy 98.45\n",
      "Got 22431/22784 with accuracy 98.45\n",
      "Got 22682/23040 with accuracy 98.45\n",
      "Got 22928/23296 with accuracy 98.42\n",
      "Got 23177/23552 with accuracy 98.41\n",
      "Got 23431/23808 with accuracy 98.42\n",
      "Got 23685/24064 with accuracy 98.43\n",
      "Got 23937/24320 with accuracy 98.43\n",
      "Got 24190/24576 with accuracy 98.43\n",
      "Got 24443/24832 with accuracy 98.43\n",
      "Got 24694/25088 with accuracy 98.43\n",
      "Got 24948/25344 with accuracy 98.44\n",
      "Got 25202/25600 with accuracy 98.45\n",
      "Got 25454/25856 with accuracy 98.45\n",
      "Got 25706/26112 with accuracy 98.45\n",
      "Got 25960/26368 with accuracy 98.45\n",
      "Got 26213/26624 with accuracy 98.46\n",
      "Got 26464/26880 with accuracy 98.45\n",
      "Got 26712/27136 with accuracy 98.44\n",
      "Got 26963/27392 with accuracy 98.43\n",
      "Got 27218/27648 with accuracy 98.44\n",
      "Got 27470/27904 with accuracy 98.44\n",
      "Got 27720/28160 with accuracy 98.44\n",
      "Got 27971/28416 with accuracy 98.43\n",
      "Got 28219/28672 with accuracy 98.42\n",
      "Got 28471/28928 with accuracy 98.42\n",
      "Got 28722/29184 with accuracy 98.42\n",
      "Got 28971/29440 with accuracy 98.41\n",
      "Got 29223/29696 with accuracy 98.41\n",
      "Got 29475/29952 with accuracy 98.41\n",
      "Got 29725/30208 with accuracy 98.40\n",
      "Got 29980/30464 with accuracy 98.41\n",
      "Got 30233/30720 with accuracy 98.41\n",
      "Got 30487/30976 with accuracy 98.42\n",
      "Got 30740/31232 with accuracy 98.42\n",
      "Got 30991/31488 with accuracy 98.42\n",
      "Got 31003/31500 with accuracy 98.42\n"
     ]
    }
   ],
   "source": [
    "check_acc(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T03:55:23.631934Z",
     "iopub.status.busy": "2021-12-10T03:55:23.631278Z",
     "iopub.status.idle": "2021-12-10T03:55:24.3441Z",
     "shell.execute_reply": "2021-12-10T03:55:24.343215Z",
     "shell.execute_reply.started": "2021-12-10T03:55:23.631891Z"
    },
    "id": "lwHWKv10ct9b",
    "outputId": "1eb4af3e-0120-4e56-db82-0a9bc35b22cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 242/256 with accuracy 94.53\n",
      "Got 490/512 with accuracy 95.70\n",
      "Got 737/768 with accuracy 95.96\n",
      "Got 987/1024 with accuracy 96.39\n",
      "Got 1240/1280 with accuracy 96.88\n",
      "Got 1489/1536 with accuracy 96.94\n",
      "Got 1738/1792 with accuracy 96.99\n",
      "Got 1988/2048 with accuracy 97.07\n",
      "Got 2238/2304 with accuracy 97.14\n",
      "Got 2488/2560 with accuracy 97.19\n",
      "Got 2739/2816 with accuracy 97.27\n",
      "Got 2990/3072 with accuracy 97.33\n",
      "Got 3240/3328 with accuracy 97.36\n",
      "Got 3487/3584 with accuracy 97.29\n",
      "Got 3738/3840 with accuracy 97.34\n",
      "Got 3991/4096 with accuracy 97.44\n",
      "Got 4240/4352 with accuracy 97.43\n",
      "Got 4495/4608 with accuracy 97.55\n",
      "Got 4744/4864 with accuracy 97.53\n",
      "Got 4998/5120 with accuracy 97.62\n",
      "Got 5247/5376 with accuracy 97.60\n",
      "Got 5498/5632 with accuracy 97.62\n",
      "Got 5747/5888 with accuracy 97.61\n",
      "Got 5997/6144 with accuracy 97.61\n",
      "Got 6248/6400 with accuracy 97.62\n",
      "Got 6499/6656 with accuracy 97.64\n",
      "Got 6751/6912 with accuracy 97.67\n",
      "Got 7003/7168 with accuracy 97.70\n",
      "Got 7253/7424 with accuracy 97.70\n",
      "Got 7507/7680 with accuracy 97.75\n",
      "Got 7760/7936 with accuracy 97.78\n",
      "Got 8012/8192 with accuracy 97.80\n",
      "Got 8264/8448 with accuracy 97.82\n",
      "Got 8517/8704 with accuracy 97.85\n",
      "Got 8769/8960 with accuracy 97.87\n",
      "Got 9020/9216 with accuracy 97.87\n",
      "Got 9273/9472 with accuracy 97.90\n",
      "Got 9526/9728 with accuracy 97.92\n",
      "Got 9777/9984 with accuracy 97.93\n",
      "Got 10031/10240 with accuracy 97.96\n",
      "Got 10279/10496 with accuracy 97.93\n",
      "Got 10283/10500 with accuracy 97.93\n"
     ]
    }
   ],
   "source": [
    "check_acc(val_loader, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T04:03:14.153979Z",
     "iopub.status.busy": "2021-12-10T04:03:14.153697Z",
     "iopub.status.idle": "2021-12-10T04:03:14.158271Z",
     "shell.execute_reply": "2021-12-10T04:03:14.157603Z",
     "shell.execute_reply.started": "2021-12-10T04:03:14.153945Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loader=DataLoader(x_test,batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T04:04:49.108254Z",
     "iopub.status.busy": "2021-12-10T04:04:49.107963Z",
     "iopub.status.idle": "2021-12-10T04:05:01.329610Z",
     "shell.execute_reply": "2021-12-10T04:05:01.328717Z",
     "shell.execute_reply.started": "2021-12-10T04:04:49.108222Z"
    }
   },
   "outputs": [],
   "source": [
    "result=pd.DataFrame(columns=['ImageId','Label'])\n",
    "serial=[]\n",
    "ans=[]\n",
    "model.eval()\n",
    "for s,i in enumerate(test_loader):\n",
    "    serial.append(s+1)\n",
    "    ans.append(model(i).argmax())\n",
    "result['ImageId']=serial    \n",
    "result['Label']=np.array(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T04:05:30.591419Z",
     "iopub.status.busy": "2021-12-10T04:05:30.590846Z",
     "iopub.status.idle": "2021-12-10T04:05:30.650194Z",
     "shell.execute_reply": "2021-12-10T04:05:30.649317Z",
     "shell.execute_reply.started": "2021-12-10T04:05:30.591367Z"
    }
   },
   "outputs": [],
   "source": [
    "result.to_csv(\"result.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T04:05:38.315723Z",
     "iopub.status.busy": "2021-12-10T04:05:38.315418Z",
     "iopub.status.idle": "2021-12-10T04:05:38.333904Z",
     "shell.execute_reply": "2021-12-10T04:05:38.333086Z",
     "shell.execute_reply.started": "2021-12-10T04:05:38.315690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      0\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
